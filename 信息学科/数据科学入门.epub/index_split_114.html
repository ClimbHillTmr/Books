<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos495973" class="calibre_"><span class="calibre6"><span class="bold"> 16.3　应用模型 </span></span></p><p class="calibre_">现在我们要将数据分为一个训练集和一个测试集：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>random.seed(0)<br class="calibre12"/>x_train, x_test, y_train, y_test = train_test_split(rescaled_x, y, 0.33)<br class="calibre12"/><br class="calibre12"/># 希望在训练数据集上最大化对数似然<br class="calibre12"/>fn = partial(logistic_log_likelihood, x_train, y_train)<br class="calibre12"/>gradient_fn = partial(logistic_log_gradient, x_train, y_train)<br class="calibre12"/><br class="calibre12"/># 选取一个随机起始点<br class="calibre12"/>beta_0 = [random.random() for _ in range(3)]<br class="calibre12"/><br class="calibre12"/># 使用梯度下降法实现最大化<br class="calibre12"/>beta_hat = maximize_batch(fn, gradient_fn, beta_0)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">另外，你也可以使用随机梯度下降法：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>beta_hat = maximize_stochastic(logistic_log_likelihood_i,<br class="calibre12"/>                               logistic_log_gradient_i,<br class="calibre12"/>                               x_train, y_train, beta_0)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">无论使用哪种方式，我们都能得到大致如下的结果：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>beta_hat = [-1.90, 4.05, -3.87]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这些数据是按照某些系数转换过来的，不过，我们还可以将其转换为原始数据：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>beta_hat_unscaled = [7.61, 1.42, -0.000249]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">令人遗憾的是，这些系数不如线性回归系数那样易于解释。在其他条件都相同的情况下，工作年限每增加一年，<tt class="calibre7">logistic</tt> 的输入就会增加 1.42。在其他条件都相同的情况下，年薪每增加 10 000 美元，<tt class="calibre7">logistic</tt> 的输入就会减去 2.49。</p><p class="calibre_">然而，输出的结果还会受到其他输入数据的影响。如果 <tt class="calibre7">dot (beta, x_i)</tt> 的值已经很大了（相当于概率接近 1），那么即使再增加的话，对概率也没有多大的影响了。如果它接近 0，那么即使稍微增加一点，对概率也会产生明显的影响。</p><p class="calibre_">也就是说，在其他条件相同的情况下，工作年限越多的人越有可能成为付费用户。同时，其他条件相同的情况下，年薪越高的人越不可能成为付费用户。（当我们将数据绘成图表的时候，这一点就会更明显。）</p><div class="mbp_pagebreak" id="calibre_pb_114"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
