<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos272750" class="calibre_15"><span class="calibre6"><span class="bold"> 8.5　综合 </span></span></p><p class="calibre_">通常而言，我们有一些 <tt class="calibre7">target_fn</tt> 函数，需要对其进行最小化，也有梯度函数 <tt class="calibre7">gradient_fn</tt> 。比如，函数 <tt class="calibre7">target_fn</tt> 可能代表模型的残差，它是参数的函数。我们可能需要找到能使残差尽可能小的参数。</p><p class="calibre_">此外，假设我们（以某种方式）为参数 <tt class="calibre7">theta_0</tt> 设定了某个初始值，那么可以如下使用梯度下降法：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):<br class="calibre12"/>    """use gradient descent to find theta that minimizes target function"""<br class="calibre12"/><br class="calibre12"/>    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]<br class="calibre12"/><br class="calibre12"/>    theta = theta_0                           # 设定theta为初始值<br class="calibre12"/>    target_fn = safe(target_fn)               # target_fn的安全版<br class="calibre12"/>    value = target_fn(theta)                  # 我们试图最小化的值<br class="calibre12"/><br class="calibre12"/>    while True:<br class="calibre12"/>        gradient = gradient_fn(theta)<br class="calibre12"/>        next_thetas = [step(theta, gradient, -step_size)<br class="calibre12"/>                       for step_size in step_sizes]<br class="calibre12"/><br class="calibre12"/>    # 选择一个使残差函数最小的值<br class="calibre12"/>    next_theta = min(next_thetas, key=target_fn)<br class="calibre12"/>    next_value = target_fn(next_theta)<br class="calibre12"/><br class="calibre12"/>    # 当“收敛”时停止<br class="calibre12"/>    if abs(value - next_value) &lt; tolerance:<br class="calibre12"/>        return theta<br class="calibre12"/>    else:<br class="calibre12"/>        theta, value = next_theta, next_value<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们称它为 <tt class="calibre7">minimize_batch</tt> ，因为在每一步梯度计算中，它都会搜索整个数据集（因为 <tt class="calibre7">target_fn</tt> 代表整个数据集的残差）。在下一部分中，我们会探讨另一种方法，一次仅考虑一个数据点。</p><p class="calibre_">有时候，我们需要<span class="bold">最大化</span> 某个函数，这只需要最小化这个函数的负值（相应的梯度函数也需取负）：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def negate(f):<br class="calibre12"/>    """return a function that for any input x returns -f(x)"""<br class="calibre12"/>    return lambda *args, **kwargs: -f(*args, **kwargs)<br class="calibre12"/><br class="calibre12"/>def negate_all(f):<br class="calibre12"/>    """the same when f returns a list of numbers"""<br class="calibre12"/>    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]<br class="calibre12"/><br class="calibre12"/>def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):<br class="calibre12"/>    return minimize_batch(negate(target_fn),<br class="calibre12"/>                          negate_all(gradient_fn),<br class="calibre12"/>                          theta_0,<br class="calibre12"/>                          tolerance)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><div class="mbp_pagebreak" id="calibre_pb_60"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
