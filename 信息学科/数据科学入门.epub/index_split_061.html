<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos276087" class="calibre_15"><span class="calibre6"><span class="bold"> 8.6　随机梯度下降法 </span></span></p><p class="calibre_">正如之前提到的，我们常常用梯度下降的方法，通过最小化某种形式的残差来选择模型参数。如果使用之前的批处理方法，每个梯度计算步都需要我们预测并计算整个数据集的梯度，这使每一步都会耗费很长时间。</p><p class="calibre_">现在，这些残差函数常常具有<span class="bold">可加性</span> （additive），意味着整个数据集上的预测残差恰好是每个数据点的预测残差之和。</p><p class="calibre_">在这种情形下，我们转而使用一种称为<span class="bold">随机梯度下降</span> （stochastic gradient descent）的技术，它每次仅计算一个点的梯度（并向前跨一步）。这个计算会反复循环，直到达到一个停止点。</p><p class="calibre_">在每个循环中，我们都会在整个数据集上按照一个随机序列迭代：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def in_random_order(data):<br class="calibre12"/>    """generator that returns the elements of data in random order"""<br class="calibre12"/>    indexes = [i for i, _ in enumerate(data)]  # 生成索引列表<br class="calibre12"/>    random.shuffle(indexes)                    # 随机打乱数据<br class="calibre12"/>    for i in indexes:                          # 返回序列中的数据<br class="calibre12"/>        yield data[i]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们对每个数据点都会进行一步梯度计算。这种方法留有这样一种可能性，即也许会在最小值附近一直循环下去，所以，每当停止获得改进，我们都会减小步长并最终退出：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):<br class="calibre12"/><br class="calibre12"/>    data = zip(x, y)<br class="calibre12"/>    theta = theta_0                             # 初始值猜测<br class="calibre12"/>    alpha = alpha_0                             # 初始步长<br class="calibre12"/>    min_theta, min_value = None, float("inf")   # 迄今为止的最小值<br class="calibre12"/>    iterations_with_no_improvement = 0<br class="calibre12"/><br class="calibre12"/>    # 如果循环超过100次仍无改进，停止<br class="calibre12"/>    while iterations_with_no_improvement &lt; 100:<br class="calibre12"/>        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )<br class="calibre12"/><br class="calibre12"/>        if value &lt; min_value:<br class="calibre12"/>            # 如果找到新的最小值，记住它<br class="calibre12"/>            # 并返回到最初的步长<br class="calibre12"/>            min_theta, min_value = theta, value<br class="calibre12"/>            iterations_with_no_improvement = 0<br class="calibre12"/>            alpha = alpha_0<br class="calibre12"/>        else:<br class="calibre12"/>            # 尝试缩小步长，否则没有改进<br class="calibre12"/>            iterations_with_no_improvement += 1<br class="calibre12"/>            alpha *= 0.9<br class="calibre12"/><br class="calibre12"/>        # 在每个数据点上向梯度方向前进一步<br class="calibre12"/>        for x_i, y_i in in_random_order(data):<br class="calibre12"/>            gradient_i = gradient_fn(x_i, y_i, theta)<br class="calibre12"/>            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))<br class="calibre12"/><br class="calibre12"/>    return min_theta<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">随机化通常比批处理化快很多。当然，我们也希望获得最大化的结果：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):<br class="calibre12"/>    return minimize_stochastic(negate(target_fn),<br class="calibre12"/>                               negate_all(gradient_fn),<br class="calibre12"/>                               x, y, theta_0, alpha_0)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><div class="mbp_pagebreak" id="calibre_pb_61"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
