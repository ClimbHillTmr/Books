<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos622403" class="calibre_"><span class="calibre6"><span class="bold"> 20.5　主题建模 </span></span></p><p class="calibre_">在第 1 章我们建立“你应该知道的数据科学家”推荐系统的时候，我们只是根据科学家与读者的兴趣是否严格匹配来进行推荐的。</p><p class="calibre_">要想理解我们的用户，更高级的方法是识别这些兴趣背后的相关主题。有一种叫作<span class="bold">隐含狄利克雷分析</span> （latent dirichlet analysis，LDA）的技术，常用来确定一组文档的共同主题。我们会用它来分析包含每个用户的兴趣的那些文档。</p><p class="calibre_">这里的 LDA 与第 13 章中的朴素贝叶斯分类器有一些相似之处，它们都是用于处理文档的概率模型。我们将尽量避开一些数学细节问题，但对于该模型的某些假设却不得不说，如下所述。</p><div class="calibre_5"> </div><ul class="calibre_6"><li value="1" class="calibre_7"><p class="calibre_">存在固定数目的主题，即 <span class="italic">K</span> 个。</p></li><li value="2" class="calibre_8"><p class="calibre_">有一个给每个主题在单词上的概率分布赋值的随机变量。你可以把这个分布看作是单词 <span class="italic">w</span> 在给定主题 <span class="italic">k</span> 中出现的概率。</p></li><li value="3" class="calibre_8"><p class="calibre_">还有另一个随机变量来指出每个文档在主题下面的概率分布。你可以将这个分布看作是文档 <span class="italic">d</span> 中各主题所占比重。</p></li><li value="4" class="calibre_8"><p class="calibre_">文档中各个单词的生成方式为，首先（根据文档的主题分布情况）随机选择一个主题，然后（根据该主题下面各单词的分布情况）随机选择一个单词。</p></li></ul><p class="calibre_">特别地，我们要建立一个文档（<tt class="calibre7">documents</tt> ）集合，其中每个文档都是一个单词的列表。</p><p class="calibre_">同时，我们还要建立一个相应的 <tt class="calibre7">document_topics</tt> 集合，以便给每个文档中的每个单词都指定一个主题（这里用 0 到 <span class="italic">K</span> –1 之间的一个数字表示）。</p><p class="calibre_">这样的话，第 4 个文档中的第 5 个单词就可以表示为：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>documents[3][4]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">而这个选定的单词对应的主题可以表示为：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>document_topics[3][4]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这非常显式地定义了每个文档在各个主题上的分布情况，同时也隐式地定义了每个主题在各个单词上面的分布情况。</p><p class="calibre_">我们可以估算出主题 1 产生一个特定单词的可能性，方法是将主题 1 产生该单词的次数除以主题 1 产生任意单词的次数。（类似地，我们在第 13 章中建立垃圾邮件过滤器时，也曾经用每个单词出现在垃圾邮件中的次数与这些单词出现在垃圾邮件中的总次数进行过相应的比较。）</p><p class="calibre_">这些主题都只是些数字，不过，我们可以用其权重最大的单词给它们取一个描述性的名称。接下来，我们只需设法生成 <tt class="calibre7">document_topics</tt> 即可。这时，吉布斯取样技术就派上用场了。</p><p class="calibre_">首先，我们以完全随机的方式给所有文档中的每个单词都赋予一个主题。现在，我们就以每次一个单词的方式遍历所有文档。对于给定的单词和文档，我们需要根据该文档中主题（当前）的分布情况以及相对于该主题各单词（当前）的分布情况来建立相应的权重。然后，我们会使用这些权重给这个单词选取新主题。如果将该过程迭代多次，我们就能利用主题 - 单词分布和文档 - 主题分布完成联合取样。</p><p class="calibre_">首先，我们需要一个函数，根据任意一组权重来随机选择一个索引：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def sample_from(weights):<br class="calibre12"/>    """returns i with probability weights[i] / sum(weights)"""<br class="calibre12"/>    total = sum(weights)<br class="calibre12"/>    rnd = total * random.random()      # 在0和总数之间均匀分布<br class="calibre12"/>    for i, w in enumerate(weights):<br class="calibre12"/>        rnd -= w                       # 返回最小的i<br class="calibre12"/>        if rnd &lt;= 0: return i          # 因此weights[0] + … + weights[i] &gt;= rnd<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">例如，如果你给它的权重为 [1, 3, 1]，那么五分之一的时间将返回 0，五分之一的时间将返回 1，同时还有五分之三的时间将返回 2。</p><p class="calibre_">我们的文档包含的是用户的各种兴趣，看起来可能像下面这样：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>documents = [<br class="calibre12"/>    ["Hadoop", "Big Data", "HBase", "Java", "Spark", "Storm", "Cassandra"],<br class="calibre12"/>    ["NoSQL", "MongoDB", "Cassandra", "HBase", "Postgres"],<br class="calibre12"/>    ["Python", "scikit-learn", "scipy", "numpy", "statsmodels", "pandas"],<br class="calibre12"/>    ["R", "Python", "statistics", "regression", "probability"],<br class="calibre12"/>    ["machine learning", "regression", "decision trees", "libsvm"],<br class="calibre12"/>    ["Python", "R", "Java", "C++", "Haskell", "programming languages"],<br class="calibre12"/>    ["statistics", "probability", "mathematics", "theory"],<br class="calibre12"/>    ["machine learning", "scikit-learn", "Mahout", "neural networks"],<br class="calibre12"/>    ["neural networks", "deep learning", "Big Data", "artificial intelligence"],<br class="calibre12"/>    ["Hadoop", "Java", "MapReduce", "Big Data"],<br class="calibre12"/>    ["statistics", "R", "statsmodels"],<br class="calibre12"/>    ["C++", "deep learning", "artificial intelligence", "probability"],<br class="calibre12"/>    ["pandas", "R", "Python"],<br class="calibre12"/>    ["databases", "HBase", "Postgres", "MySQL", "MongoDB"],<br class="calibre12"/>    ["libsvm", "regression", "support vector machines"]<br class="calibre12"/>]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们将设法找出 4 个主题，即 <tt class="calibre7">K = 4</tt> 。</p><p class="calibre_">为了计算抽样权重，我们需要明确几项计数。下面，我们先给它们创建相应的数据结构。</p><p class="calibre_">我们要统计每个文档中每个主题出现的次数，代码如下：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 计数的一个列表，每个文档各有一个列表<br class="calibre12"/>document_topic_counts = [Counter() for _ in documents]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们要统计每个主题中每个单词出现的次数，代码如下：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 计数的一个列表，每个主题各有一个列表<br class="calibre12"/>topic_word_counts = [Counter() for _ in range(K)]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们要知道每个主题中单词的总数，代码如下：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 数字的一个列表, 每个主题各有一个列表<br class="calibre12"/>topic_counts = [0 for _ in range(K)]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">下面的代码统计每个文档中的单词总数：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 数字的一个列表，每个文档各有一个列表<br class="calibre12"/>document_lengths = map(len, documents)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">下面代码统计不同单词的数量：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>distinct_words = set(word for document in documents for word in document)<br class="calibre12"/>W = len(distinct_words)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">另外，下列代码可以用来统计文档的数量：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>D = len(documents)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">一旦掌握了这些数据，我们就可以了解（比如说）<tt class="calibre7">documents[3]</tt> 中与主题 1 相关的单词的数量，具体代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>document_topic_counts[3][1]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">同时，我们还可以找出与主题 2 相关的单词 nlp 出现的次数，具体代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>topic_word_counts[2]["nlp"]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">现在，我们已经为定义条件概率函数做好了准备。就像在第 13 章中那样，这里的每个主题和单词都需要有一个平滑项，来确保每个主题在任何文档中被选中的几率都不能为 0，同时保证每个单词在任何主题中被选中的几率也都不能为 0：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def p_topic_given_document(topic, d, alpha=0.1):<br class="calibre12"/>    """the fraction of words in document _d_<br class="calibre12"/>    that are assigned to _topic_ (plus some smoothing)"""<br class="calibre12"/><br class="calibre12"/>    return ((document_topic_counts[d][topic] + alpha) /<br class="calibre12"/>            (document_lengths[d] + K * alpha))<br class="calibre12"/><br class="calibre12"/>def p_word_given_topic(word, topic, beta=0.1):<br class="calibre12"/>    """the fraction of words assigned to _topic_<br class="calibre12"/>    that equal _word_ (plus some smoothing)"""<br class="calibre12"/><br class="calibre12"/>return ((topic_word_counts[topic][word] + beta) /<br class="calibre12"/>        (topic_counts[topic] + W * beta))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">然后利用下列代码给更新中的主题确定权重：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def topic_weight(d, word, k):<br class="calibre12"/>    """given a document and a word in that document,<br class="calibre12"/>    return the weight for the kth topic"""<br class="calibre12"/>    return p_word_given_topic(word, k) * p_topic_given_document(k, d)<br class="calibre12"/><br class="calibre12"/>def choose_new_topic(d, word):<br class="calibre12"/>    return sample_from([topic_weight(d, word, k)<br class="calibre12"/>                        for k in range(K)])<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">上面的 topic_weight 之所以如此定义，背后是有坚实的数学理论作为依据的，不过其中的数学细节已经超出了本书的讨论范围。不过从直观的角度来看，如果已知一个单词和所在文档，那么该单词属于某主题的概率取决于两个方面，即该主题属于该文档的可能性以及该单词属于该主题的可能性。</p><p class="calibre_">这就是我们所需要的全部零部件。下面，我们开始将每个单词随机指派给一个话题，并计入相应的计数器：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>random.seed(0)<br class="calibre12"/>document_topics = [[random.randrange(K) for word in document]<br class="calibre12"/>                   for document in documents]<br class="calibre12"/><br class="calibre12"/>for d in range(D):<br class="calibre12"/>    for word, topic in zip(documents[d], document_topics[d]):<br class="calibre12"/>        document_topic_counts[d][topic] += 1<br class="calibre12"/>        topic_word_counts[topic][word] += 1<br class="calibre12"/>        topic_counts[topic] += 1<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们的目标是利用主题 - 单词分布和文档 - 主题分布进行联合采样。为此，我们可以通过之前定义的条件概率进行吉布斯采样，具体代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>for iter in range(1000):<br class="calibre12"/>    for d in range(D):<br class="calibre12"/>        for i, (word, topic) in enumerate(zip(documents[d],<br class="calibre12"/>                                              document_topics[d])):<br class="calibre12"/><br class="calibre12"/>            # 从计数中移除这个单词/主题<br class="calibre12"/>            # 以便它不会影响权重<br class="calibre12"/>            document_topic_counts[d][topic] -= 1<br class="calibre12"/>            topic_word_counts[topic][word] -= 1<br class="calibre12"/>            topic_counts[topic] -= 1<br class="calibre12"/>            document_lengths[d] -= 1<br class="calibre12"/><br class="calibre12"/>            # 基于权重选择一个新的主题<br class="calibre12"/>            new_topic = choose_new_topic(d, word)<br class="calibre12"/>            document_topics[d][i] = new_topic<br class="calibre12"/><br class="calibre12"/>            # 现在把它重新加到计数中<br class="calibre12"/>            document_topic_counts[d][new_topic] += 1<br class="calibre12"/>            topic_word_counts[new_topic][word] += 1<br class="calibre12"/>            topic_counts[new_topic] += 1<br class="calibre12"/>            document_lengths[d] += 1<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这些主题都是什么呢？它们只是些数字而已：0、1、2 和 3。如果我们要让它们拥有名称的话，必须亲自给它们取名。下面，让我们来找出权重较大的 5 个单词（见表 20-1），具体代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>for k, word_counts in enumerate(topic_word_counts):<br class="calibre12"/>    for word, count in word_counts.most_common():<br class="calibre12"/>        if count &gt; 0: print k, word, count<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">
<span class="bold">表20-1：每个主题中最常见的单词</span>
</p><p class="calibre13"> </p><table border="1" valign="top" class="calibre_37"><tr valign="top" class="calibre14"><th valign="top" class="calibre15"><span class="calibre11"><span class="bold">  </span></span><p class="calibre_38"><span class="calibre11"><span class="bold">主题0 </span></span></p></th><th valign="top" class="calibre15"><span class="calibre11"><span class="bold">  </span></span><p class="calibre_38"><span class="calibre11"><span class="bold">主题1 </span></span></p></th><th valign="top" class="calibre15"><span class="calibre11"><span class="bold">  </span></span><p class="calibre_38"><span class="calibre11"><span class="bold">主题2 </span></span></p></th><th valign="top" class="calibre15"><span class="calibre11"><span class="bold">  </span></span><p class="calibre_38"><span class="calibre11"><span class="bold">主题3 </span></span></p></th></tr><tr valign="top" class="calibre14"><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Java </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">R </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">HBase </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">regression </span></p></td></tr><tr valign="top" class="calibre_40"><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Big Data </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">statistics </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Postgres </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">libsvm </span></p></td></tr><tr valign="top" class="calibre14"><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Hadoop </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Python </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">MongoDB </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">scikit-learn </span></p></td></tr><tr valign="top" class="calibre_40"><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">deep learning </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">probability </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Cassandra </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">machine learning </span></p></td></tr><tr valign="top" class="calibre14"><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">artificial intelligence </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">Pandas </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">NoSQL </span></p></td><td class="calibre16"><span class="calibre11">  </span><p class="calibre_39"><span class="calibre11">neural networks </span></p></td></tr></table><p class="calibre_">根据这些数据，我们就可以给主题取名了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>topic_names = ["Big Data and programming languages",<br class="calibre12"/>               "Python and statistics",<br class="calibre12"/>               "databases",<br class="calibre12"/>               "machine learning"]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">至此我们就清楚模型是如何将主题分配到每个用户的兴趣上面了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>for document, topic_counts in zip(documents, document_topic_counts):<br class="calibre12"/>    print document<br class="calibre12"/>    for topic, count in topic_counts.most_common():<br class="calibre12"/>        if count &gt; 0:<br class="calibre12"/>            print topic_names[topic], count,<br class="calibre12"/>    print<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">其输出结果如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']<br class="calibre12"/>Big Data and programming languages 4 databases 3<br class="calibre12"/>['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']<br class="calibre12"/>databases 5<br class="calibre12"/>['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']<br class="calibre12"/>Python and statistics 5 machine learning 1<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">如此等等。假使我们被要求用“ands”作为某个主题的名称，那么我们很可能需要使用更多的主题，尽管更可能的情况是我们没有足够的数据来学习。</p><div class="mbp_pagebreak" id="calibre_pb_145"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
