<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos518307" class="calibre_"><span class="calibre6"><span class="bold"> 17.4　创建决策树 </span></span></p><p class="calibre_">副总为你提供了应聘者的相关资料，包括符合你的数据规范的 <tt class="calibre7">(input, label)</tt> 对，其中每个输入都是由应聘者的各种属性构成的一个字典变量，而每个标签的取值要么为 <tt class="calibre7">True</tt> （该求职者面试成绩很好），要么为 <tt class="calibre7">False</tt> （该求职者面试成绩很差）。具体来说，数据为你提供了每个求职者的得分情况、常用语言、在 Twitter 上的活跃程度以及是否拥有博士学位等信息：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>inputs = [<br class="calibre12"/>    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'no'},    False),<br class="calibre12"/>    ({'level':'Senior', 'lang':'Java', 'tweets':'no', 'phd':'yes'},   False),<br class="calibre12"/>    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'no'},      True),<br class="calibre12"/>    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'no'},   True),<br class="calibre12"/>    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'no'},       True),<br class="calibre12"/>    ({'level':'Junior', 'lang':'R', 'tweets':'yes', 'phd':'yes'},     False),<br class="calibre12"/>    ({'level':'Mid', 'lang':'R', 'tweets':'yes', 'phd':'yes'},         True),<br class="calibre12"/>    ({'level':'Senior', 'lang':'Python', 'tweets':'no', 'phd':'no'},  False),<br class="calibre12"/>    ({'level':'Senior', 'lang':'R', 'tweets':'yes', 'phd':'no'},       True),<br class="calibre12"/>    ({'level':'Junior', 'lang':'Python', 'tweets':'yes', 'phd':'no'},  True),<br class="calibre12"/>    ({'level':'Senior', 'lang':'Python', 'tweets':'yes', 'phd':'yes'}, True),<br class="calibre12"/>    ({'level':'Mid', 'lang':'Python', 'tweets':'no', 'phd':'yes'},     True),<br class="calibre12"/>    ({'level':'Mid', 'lang':'Java', 'tweets':'yes', 'phd':'no'},       True),<br class="calibre12"/>    ({'level':'Junior', 'lang':'Python', 'tweets':'no', 'phd':'yes'}, False)<br class="calibre12"/>]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们的决策树含有许多<span class="bold">决策节点</span> （该节点会提出一个问题，并根据问题的答案来指导我们下一步如何走）和<span class="bold">叶节点</span> （该节点为我们提供预测结果）。我们将使用较为简单的 ID3 算法来创建决策树，具体过程将在下面详细介绍。假设我们得到了一些标记过的数据，以及一个用来选择下一个分支的属性列表。</p><div class="calibre_5"> </div><ul class="calibre_6"><li value="1" class="calibre_7"><p class="calibre_">如果所有数据都有相同的标签，那么创建一个预测最终结果即为该标签所示的叶节点，然后停止。</p></li><li value="2" class="calibre_8"><p class="calibre_">如果属性列表是空的（即已经没有更多的问题可提问了），就创建一个预测结果为最常见的标签的叶节点，然后停止。</p></li><li value="3" class="calibre_8"><p class="calibre_">否则，尝试用每个属性对数据进行划分。</p></li><li value="4" class="calibre_8"><p class="calibre_">选择具有最低划分熵的那次划分的结果。</p></li><li value="5" class="calibre_8"><p class="calibre_">根据选定的属性添加一个决策节点。</p></li><li value="6" class="calibre_8"><p class="calibre_">针对划分得到的每个子集，利用剩余属性重复上述过程。</p></li></ul><p class="calibre_">这就是所谓的“贪婪”算法，因为在每一步，它都会选择最快最好的那一个。对于特定的数据集，有的决策树在开头几步看起来表现不佳，但最终结果却可能是最棒的。如果是这样的话，这个算法就无法找到这样的决策树。尽管如此，它也有自己的优点，即相对来说还是很容易理解和实现的，所以，把它用作探索决策树的起点还是非常不错的。</p><p class="calibre_">下面让我们以手动方式在应聘者数据集上完成这些步骤。这个数据集具有 <tt class="calibre7">True</tt> 和 <tt class="calibre7">False</tt> 两种标签，我们将利用 4 个属性对其进行分类。因此，我们首先要做的就是找出熵最小的分割方法。我们将通过如下函数来完成分割：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def partition_by(inputs, attribute):<br class="calibre12"/>    """each input is a pair (attribute_dict, label).<br class="calibre12"/>    returns a dict : attribute_value -&gt; inputs"""<br class="calibre12"/>    groups = defaultdict(list)<br class="calibre12"/>    for input in inputs:<br class="calibre12"/>        key = input[0][attribute]   # 得到特定属性的值<br class="calibre12"/>        groups[key].append(input)   # 然后把这个输入加到正确的列表中<br class="calibre12"/>    return groups<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们可以通过下列代码来计算熵：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def partition_entropy_by(inputs, attribute):<br class="calibre12"/>    """computes the entropy corresponding to the given partition"""<br class="calibre12"/>    partitions = partition_by(inputs, attribute)<br class="calibre12"/>    return partition_entropy(partitions.values())<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">然后我们只需要找出在整个数据集上具有最小熵的分割即可：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>for key in ['level','lang','tweets','phd']:<br class="calibre12"/>    print key, partition_entropy_by(inputs, key)<br class="calibre12"/><br class="calibre12"/># level 0.693536138896<br class="calibre12"/># lang 0.860131712855<br class="calibre12"/># tweets 0.788450457308<br class="calibre12"/># phd 0.892158928262<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们看到，利用 <tt class="calibre7">level</tt> 进行的分割的熵最小，所以我们需要为每一个可能的 <tt class="calibre7">level</tt> 值建立一个子树。所有 <tt class="calibre7">Mid</tt> 应聘者都被标记成了 <tt class="calibre7">True</tt> ，这意味着 <tt class="calibre7">Mid</tt> 子树是一个叶节点，其预测结果为 <tt class="calibre7">True</tt> 。对于 Senior 级别的求职者，其标签既有 <tt class="calibre7">True</tt> 也有 <tt class="calibre7">False</tt> ，所以我们需要进一步划分：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>senior_inputs = [(input, label)<br class="calibre12"/>                 for input, label in inputs if input["level"] == "Senior"]<br class="calibre12"/><br class="calibre12"/>for key in ['lang', 'tweets', 'phd']:<br class="calibre12"/>    print key, partition_entropy_by(senior_inputs, key)<br class="calibre12"/><br class="calibre12"/># lang 0.4<br class="calibre12"/># tweets 0.0<br class="calibre12"/># phd 0.950977500433<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这表明，我们下一步应该根据 <tt class="calibre7">tweets</tt> 进行分割，因为它能得到熵为 0 的分割。对于 Senior 级别的应聘者，<tt class="calibre7">tweets</tt> 的值为“yes”的最终分类结果为 <tt class="calibre7">True</tt> ，而 <tt class="calibre7">tweets</tt> 的值为“no”的最终分类结果为 <tt class="calibre7">False</tt> 。</p><p class="calibre_">最后，如果我们对 Junior 级别的应聘者做同样的事情，最终会根据属性 <tt class="calibre7">phd</tt> 进行划分，并且发现没有博士学位的结果都是 <tt class="calibre7">True</tt> ，拥有博士学位的结果都是 <tt class="calibre7">False</tt> 。</p><p class="calibre_">图 17-3 为我们展示了完整的决策树。</p><p class="calibre_12"><img src="images/00134.jpg" class="calibre_135"/>
</p><p class="calibre_">
<span class="bold">图 17-3：招聘决策树</span>
</p><div class="mbp_pagebreak" id="calibre_pb_122"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
