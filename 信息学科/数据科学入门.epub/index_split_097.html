<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos444736" class="calibre_"><span class="calibre6"><span class="bold"> 14.1　模型 </span></span></p><p class="calibre_">别忘了，我们正在探讨的是 DataSciencester 用户的朋友数量与其每天花在该网站上的时间之间的关系。我们假设，你已经说服自己结交的朋友越多会导致人们花在网站上的时间越长。</p><p class="calibre_">这时，参与部的副总要你建立一个模型来描述这种关系。既然你发现有很强的线性关系，那么自然就要从线性模型开始着手了。准确地说，假设有常数 <span class="italic">α</span> （alpha） 和 <span class="italic">β</span> （beta），使得：</p><p class="calibre_12"><img src="images/00151.jpg" class="calibre_105"/>
</p><p class="calibre_">其中，<span class="italic">y<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 是用户 <span class="italic">i</span> 每天花在网站上的分钟数，<span class="italic">x<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 是用户 <span class="italic">i</span> 已有的朋友数，而 <span class="italic">ε<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 是误差项，用来表示这个简单模型没有考虑到的其他因素，当然，误差项越小越好。</p><p class="calibre_">只要我们求出 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> ，就能轻松通过下列公式来进行预测了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def predict(alpha, beta, x_i):<br class="calibre12"/>    return beta * x_i + alpha<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">那么，该如何选择 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> 呢？实际上，只要任意选定的 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> 值，对于每个输入 <tt class="calibre7">x_i</tt> ，都能得到一个预测的输出值。由于知道实际输出值 <tt class="calibre7">y_i</tt> ，因此可以计算它们的误差：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def error(alpha, beta, x_i, y_i):<br class="calibre12"/>    """the error from predicting beta * x_i + alpha<br class="calibre12"/>    when the actual value is y_i"""<br class="calibre12"/>    return y_i - predict(alpha, beta, x_i)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">实际上，我们真正想知道的是整个数据集的总体误差情况。不过，我们不能简单地将各个误差加起来，这是因为，如果 <tt class="calibre7">x_1</tt> 预测得太高，而 <tt class="calibre7">x_2</tt> 预测得太低，那么它们的误差加在一起就会相互抵消了。</p><p class="calibre_">因此，我们要对误差的<span class="bold">平方</span> 求和：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def sum_of_squared_errors(alpha, beta, x, y):<br class="calibre12"/>    return sum(error(alpha, beta, x_i, y_i) ** 2<br class="calibre12"/>               for x_i, y_i in zip(x, y))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们可以通过<span class="bold">最小二乘法</span> 来选择 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> ，以使 <tt class="calibre7">sum_of_squared_errors</tt> 尽可能小。</p><p class="calibre_">利用微积分（或单调乏味的代数），我们就可以求出令误差最小化的 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> 了，具体代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def least_squares_fit(x, y):<br class="calibre12"/>    """given training values for x and y,<br class="calibre12"/>    find the least-squares values of alpha and beta"""<br class="calibre12"/>    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)<br class="calibre12"/>    alpha = mean(y) - beta * mean(x)<br class="calibre12"/>    return alpha, beta<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">不要忙着进行严格的数学推导，让我们先想想为什么这可能是一个合理的解决方案。实际上，选定 <tt class="calibre7">alpha</tt> 后，只要给出自变量 <tt class="calibre7">x</tt> 的平均值，我们就能预测因变量 <tt class="calibre7">y</tt> 的平均值。</p><p class="calibre_">选定了 <tt class="calibre7">beta</tt> ，就意味着输入值每增加 <tt class="calibre7">standard_deviation(x)</tt> ，预测值就会增加 <tt class="calibre7">correlation(x, y) * standard_deviation(y)</tt> 。就本例来说，如果 <tt class="calibre7">x</tt> 和 <tt class="calibre7">y</tt> 完全相关，则 <tt class="calibre7">x</tt> 每增加一个标准偏差，预测值就会增加 <tt class="calibre7">y</tt> 的一个标准偏差。当它们完全负相关的时候，预测值会随着 <tt class="calibre7">x</tt> 的增加而<span class="bold">减小</span> 。当它们的相关性为 0 时，<tt class="calibre7">beta</tt> 为 0，这意味着 <tt class="calibre7">x</tt> 的变化根本不会对预测值产生影响。</p><p class="calibre_">下面我们使用第 5 章中的异常值数据来计算这两个值：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>alpha, beta = least_squares_fit(num_friends_good, daily_minutes_good)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">计算结果是，alpha = 22.95，beta = 0.903。因此，根据我们的模型来看，具有 <tt class="calibre7">n</tt> 个好友的用户每天会在这个网站上花 <tt class="calibre7">22.95 + n * 0.903</tt> 分钟。同时，对于在 DataSciencester 上面没有朋友的用户来说，他们每天仍然会花 23 分钟泡在这个网站上。此外，用户每增加一个朋友，每天花费在这个网站上的时间就会多出一分钟左右。在图 14-1 中，我们绘制了该模型的预测线，从中可以看出模型的预测与观测数据的拟合效果。</p><p class="calibre_12"><img src="images/00154.jpg" class="calibre_85"/>
</p><p class="calibre_">
<span class="bold">图 14-1：简单线性模型</span>
</p><p class="calibre_">当然，仅仅依靠目测是不够的，我们需要一个更好的指标来评估模型对数据的拟合效果。一个常见的指标是<span class="bold">决定系数</span> （coefficient of determination）或 R <span class="bold">平方</span> ，用来表示纳入模型的自变量引起的变动占总变动的百分比：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def total_sum_of_squares(y):<br class="calibre12"/>    """the total squared variation of y_i's from their mean"""<br class="calibre12"/>    return sum(v ** 2 for v in de_mean(y))<br class="calibre12"/><br class="calibre12"/>def r_squared(alpha, beta, x, y):<br class="calibre12"/>    """the fraction of variation in y captured by the model, which equals<br class="calibre12"/>    1 - the fraction of variation in y not captured by the model"""<br class="calibre12"/><br class="calibre12"/>    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) /<br class="calibre12"/>                  total_sum_of_squares(y))<br class="calibre12"/><br class="calibre12"/>r_squared(alpha, beta, num_friends_good, daily_minutes_good)      # 0.329<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">现在，我们已经选取了令预测误差平方和最小化的 <tt class="calibre7">alpha</tt> 和 <tt class="calibre7">beta</tt> 。我们选择的是一个“总是预测 <tt class="calibre7">mean(y)</tt> ”的线性模型（即 <tt class="calibre7">alpha = mean(y)</tt> 且 <tt class="calibre7">beta = 0</tt> ），模型误差平方和正好等于其平方总和。这就意味着拟合优度（R 平方）的值为 0，表明模型（显然，在这种情况下）几乎只能预测平均值。</p><p class="calibre_">很明显，最小二乘模型最差的时候，就是误差的平方和<span class="bold">最大</span> 为平方总和的时候，也是 R 平方最小为 0 的时候。同时，因为误差的平方和至少为 0，所以 R 平方至多为 1。</p><p class="calibre_">R 平方的值越大，说明模型对数据的拟合度越高。在这里，R 平方的值为 0.329，说明模型对这些数据的拟合度不是很高，显然还有没考虑到的其他因素在起作用。</p><div class="mbp_pagebreak" id="calibre_pb_97"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
