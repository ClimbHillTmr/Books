<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos542778" class="calibre_"><span class="calibre6"><span class="bold"> 18.2　前馈神经网络 </span></span></p><p class="calibre_">大脑的拓扑结构极为复杂，我们可以近似地把它看作一个理想化的<span class="bold">前馈</span> （feed-forward）神经网络，该网络由多层构成，每层由众多神经元组成，然后逐层相连。一般情况下，前馈神经网络会有一个输入层（接收输入信号，然后无需修改直接向前馈送），一个或者多个“隐藏层”（每层都是由神经元组成，这些神经元以前一层的输出作为其输入，进行某些计算，并将结果传递给下一层），以及一个输出层（这一层提供最终输出）。</p><p class="calibre_">正如感知器那样，每个（非输入）神经元的每个输入和偏移项都会有一个权重。为简单起见，我们将偏移项放到权重向量的末尾，并且所有神经元的偏移项的输入都是 1。</p><p class="calibre_">类似感知器那样，对于每个神经元而言，其输入与权重之积需要加总处理。不同之处在于，这里不是直接输出 <tt class="calibre7">step_function</tt> 函数应用于输入与权重之积的结果，而是将其平滑处理之后，输出一个近似值。准确地说，这里使用的是 <tt class="calibre7">sigmoid</tt> 函数，见图 18-2。</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def sigmoid(t):<br class="calibre12"/>    return 1 / (1 + math.exp(-t))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_19"><img src="images/00079.jpg" class="calibre_36"/>
</p><p class="calibre_">
<span class="bold">图 18-2：sigmoid 函数</span>
</p><p class="calibre_">为什么使用 <tt class="calibre7">sigmoid</tt> 函数，而不是更为简单的 <tt class="calibre7">step_function</tt> 函数呢？因为要训练神经网络，就得使用微积分，而要使用微积分，就得使用光滑函数。我们知道，阶梯函数无法确保处处连续，但是 <tt class="calibre7">sigmoid</tt> 函数却是它们一个非常好的平滑近似函数。</p><blockquote class="calibre_14"><img src="images/00100.jpg" class="calibre_10"/> 　你可能还记得，我们在第 16 章也用过 <tt class="calibre7">sigmoid</tt> 函数，只不过当时我们称其为 <tt class="calibre7">logistic</tt> 函数。实际上，“sigmoid”指的是函数的外形，而“logistic”指的是这种特定的函数，但是人们经常将两者等价使用。</blockquote><p class="calibre_">这样，我们就能计算其输出了，代码如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def neuron_output(weights, inputs):<br class="calibre12"/>    return sigmoid(dot(weights, inputs))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">有了这个函数，我们就可以将神经元简单表示成一个权重列表，列表的长度等于神经元输入数量加 1，因为还要加上偏移项的权重。这样，神经网络就可以用各个（非输入）层组成的列表来表示，其中每一层就是该层内的神经元所组成的一个列表。</p><p class="calibre_">也就是说，神经网络可以用（权重）列表的（神经元）列表的（层）列表来表示。</p><p class="calibre_">有了这种表示方法，神经网络用起来就会非常简便：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def feed_forward(neural_network, input_vector):<br class="calibre12"/>    """takes in a neural network<br class="calibre12"/>    (represented as a list of lists of lists of weights)<br class="calibre12"/>    and returns the output from forward-propagating the input"""<br class="calibre12"/><br class="calibre12"/>    outputs = []<br class="calibre12"/><br class="calibre12"/>    # 每次处理一层<br class="calibre12"/>    for layer in neural_network:<br class="calibre12"/>        input_with_bias = input_vector + [1]                 # 增加一个偏倚输入<br class="calibre12"/>        output = [neuron_output(neuron, input_with_bias)     # 计算输出<br class="calibre12"/>                  for neuron in layer]                       # 每一个神经元<br class="calibre12"/>        outputs.append(output)                               # 记住它<br class="calibre12"/><br class="calibre12"/>        # 然后下一层的输入就是这一层的输出<br class="calibre12"/>        input_vector = output<br class="calibre12"/><br class="calibre12"/>    return outputs<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">如今，我们无需使用感知器就能建立异或门了，这样事情就变得简单多了。所以，我们只需要调整权重，就能使得 <tt class="calibre7">neuron_outputs</tt> 非常接近 1 或 0 了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>xor_network = [# hidden layer<br class="calibre12"/>               [[20, 20, -30],      # 'and'神经元<br class="calibre12"/>                [20, 20, -10]],     # 'or'神经元<br class="calibre12"/>               # output layer<br class="calibre12"/>               [[-60, 60, -30]]]    # '第二次输入不同于第一次输入'神经元<br class="calibre12"/><br class="calibre12"/>for x in [0, 1]:<br class="calibre12"/>    for y in [0, 1]:<br class="calibre12"/>        # feed_forward生成每个神经元的输出<br class="calibre12"/>        # feed_forward[-1]是输出层神经元的输出<br class="calibre12"/>        print x, y, feed_forward(xor_network,[x, y])[-1]<br class="calibre12"/><br class="calibre12"/># 0 0 [9.38314668300676e-14]<br class="calibre12"/># 0 1 [0.9999999999999059]<br class="calibre12"/># 1 0 [0.9999999999999059]<br class="calibre12"/># 1 1 [9.383146683006828e-14]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">借助于隐藏层，我们就能把一个“与”神经元和一个“或”神经元的输出馈送至“第一个输入不同于第二个输入”神经元了。这个网络所做的工作，就是判断“或运算的结果不同于与运算的结果”，这实际上就是在执行异或运算，见图 18-3。</p><p class="calibre_12"><img src="images/00109.jpg" class="calibre_137"/>
</p><p class="calibre_">
<span class="bold">图 18-3：用于实现异或运算的神经网络</span>
</p><div class="mbp_pagebreak" id="calibre_pb_128"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
