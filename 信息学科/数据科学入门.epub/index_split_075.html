<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos363613" class="calibre_"><span class="calibre6"><span class="bold"> 10.5　降维 </span></span></p><p class="calibre_">有时候，数据的“真实”（或有用的）维度与我们掌握的数据维度并不相符。比如，考虑图 10-5 中所示的数据。</p><p class="calibre_12"><img src="images/00030.jpg" class="calibre_80"/>
</p><p class="calibre_">
<span class="bold">图 10-5：坐标轴“错误”的数据</span>
</p><p class="calibre_">数据的大部分变差看起来像是沿着单个维度分布的，既不与 <span class="italic">x</span> 轴对应，也不与 <span class="italic">y</span> 轴对应。</p><p class="calibre_">当这种情形发生时，我们可以使用一种叫作<span class="bold">主成分分析</span> （principal component analysis，PCA）的技术从数据中提取出一个或多个维度，以捕获数据中尽可能多的变差。</p><blockquote class="calibre_14"><img src="images/00100.jpg" class="calibre_10"/> 　实际上，这样的技术不适用于低维数据集。降维多用于数据集的维数很高的情形，你可以通过一个小子集来抓住数据集本身的大部分变差。不过，这种情况很复杂，绝非一两章能讲得清。</blockquote><p class="calibre_">首先，我们需要将数据转换成为每个维度均值为零的形式：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def de_mean_matrix(A):<br class="calibre12"/>    """returns the result of subtracting from every value in A the mean<br class="calibre12"/>    value of its column. the resulting matrix has mean 0 in every column"""<br class="calibre12"/>    nr, nc = shape(A)<br class="calibre12"/>    column_means, _ = scale(A)<br class="calibre12"/>    return make_matrix(nr, nc, lambda i, j: A[i][j] - column_means[j])<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">（如果不这样做，应用这种技术的结果可能就只是确定数据的均值本身，而非找出数据中的变差。）</p><p class="calibre_">图 10-6 展示了去均值后的示例数据。</p><p class="calibre_12"><img src="images/00035.jpg" class="calibre_80"/>
</p><p class="calibre_">
<span class="bold">图 10-6：去均值后的数据</span>
</p><p class="calibre_">现在，已有一个去均值的矩阵 <span class="italic">X</span> ，我们想问，最能抓住数据最大变差的方向是什么？</p><p class="calibre_">具体来说，给定一个方向 <span class="italic">d</span> （一个绝对值为 1 的向量），矩阵的每行 <span class="italic">x</span> 在方向 <span class="italic">d</span> 的扩展是点积 <tt class="calibre7">dot(x, d)</tt> 。并且如果将每个非零向量 <span class="italic">w</span> 的绝对值大小调整为 1，则它们每个都决定了一个方向：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def direction(w):<br class="calibre12"/>    mag = magnitude(w)<br class="calibre12"/>    return [w_i / mag for w_i in w]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">因此，已知一个非零向量 <span class="italic">w</span> ，我们可以计算 <span class="italic">w</span> 方向上的方差：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def directional_variance_i(x_i, w):<br class="calibre12"/>    """the variance of the row x_i in the direction determined by w"""<br class="calibre12"/>    return dot(x_i, direction(w)) ** 2<br class="calibre12"/><br class="calibre12"/>def directional_variance(X, w):<br class="calibre12"/>    """the variance of the data in the direction determined w"""<br class="calibre12"/>    return sum(directional_variance_i(x_i, w)<br class="calibre12"/>               for x_i in X)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们可以找出使方差最大的那个方向。只要得到梯度函数，我们就可以通过梯度下降法计算出来：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def directional_variance_gradient_i(x_i, w):<br class="calibre12"/>    """the contribution of row x_i to the gradient of<br class="calibre12"/>    the direction-w variance"""<br class="calibre12"/>    projection_length = dot(x_i, direction(w))<br class="calibre12"/>    return [2 * projection_length * x_ij for x_ij in x_i]<br class="calibre12"/><br class="calibre12"/>def directional_variance_gradient(X, w):<br class="calibre12"/>    return vector_sum(directional_variance_gradient_i(x_i,w)<br class="calibre12"/>                      for x_i in X)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">第一主成分仅是使函数 <tt class="calibre7">directional_variance</tt> 最大化的方向：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def first_principal_component(X):<br class="calibre12"/>    guess = [1 for _ in X[0]]<br class="calibre12"/>    unscaled_maximizer = maximize_batch(<br class="calibre12"/>        partial(directional_variance, X),           # 现在是w的一个函数<br class="calibre12"/>        partial(directional_variance_gradient, X),  # 现在是w的一个函数<br class="calibre12"/>        guess)<br class="calibre12"/>    return direction(unscaled_maximizer)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">也许，你也有可能使用随机梯度下降方法：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 这里没有"y"，所以我们仅仅是传递一个Nones的向量<br class="calibre12"/># 和忽略这个输入的函数<br class="calibre12"/>def first_principal_component_sgd(X):<br class="calibre12"/>    guess = [1 for _ in X[0]]<br class="calibre12"/>    unscaled_maximizer = maximize_stochastic(<br class="calibre12"/>        lambda x, _, w: directional_variance_i(x, w),<br class="calibre12"/>        lambda x, _, w: directional_variance_gradient_i(x, w),<br class="calibre12"/>        X,<br class="calibre12"/>        [None for _ in X],   # 假的 "y"<br class="calibre12"/>        guess)<br class="calibre12"/>    return direction(unscaled_maximizer)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">对去均值的数据集，计算结果返回了方向 <tt class="calibre7">[0.924, 0.383]</tt> ，这个方向看起来捕获了数据变动的主要方向轴（图 10-7）。</p><p class="calibre_12"><img src="images/00044.jpg" class="calibre_80"/>
</p><p class="calibre_">
<span class="bold">图 10-7：第一主成分</span>
</p><p class="calibre_">一旦我们找到了第一主成分的方向，就可以将数据在这个方向上投影得到这个成分的值：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def project(v, w):<br class="calibre12"/>    """return the projection of v onto the direction w"""<br class="calibre12"/>    projection_length = dot(v, w)<br class="calibre12"/>    return scalar_multiply(projection_length, w)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">如果还想得到其他的成分，就要先从数据中移除投影：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def remove_projection_from_vector(v, w):<br class="calibre12"/>    """projects v onto w and subtracts the result from v"""<br class="calibre12"/>    return vector_subtract(v, project(v, w))<br class="calibre12"/><br class="calibre12"/>def remove_projection(X, w):<br class="calibre12"/>    """for each row of X<br class="calibre12"/>    projects the row onto w, and subtracts the result from the row"""<br class="calibre12"/>    return [remove_projection_from_vector(x_i, w) for x_i in X]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">因为这个例子中的数据集仅仅设定为二维，当移除第一主成分之后，剩下的实际上就是一个一维的成分了（图 10-8）。</p><p class="calibre_12"><img src="images/00056.jpg" class="calibre_81"/>
</p><p class="calibre_">
<span class="bold">图 10-8：移除第一主成分之后的数据</span>
</p><p class="calibre_">在这点上，我们可以通过对 <tt class="calibre7">remove_projection</tt> 的结果重复这个过程来找到其他的主成分（图 10-9）。</p><p class="calibre_12"><img src="images/00061.jpg" class="calibre_80"/>
</p><p class="calibre_">
<span class="bold">图 10-9：前两个主成分</span>
</p><p class="calibre_">在更高维的数据集中，我们可以通过迭代找到我们所需的任意数目的主成分：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def principal_component_analysis(X, num_components):<br class="calibre12"/>    components = []<br class="calibre12"/>    for _ in range(num_components):<br class="calibre12"/>        component = first_principal_component(X)<br class="calibre12"/>        components.append(component)<br class="calibre12"/>        X = remove_projection(X, component)<br class="calibre12"/><br class="calibre12"/>    return components<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">然后再将原数据<span class="bold">转换</span> 为由主成分生成的低维空间中的点：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def transform_vector(v, components):<br class="calibre12"/>    return [dot(v, w) for w in components]<br class="calibre12"/><br class="calibre12"/>def transform(X, components):<br class="calibre12"/>    return [transform_vector(x_i, components) for x_i in X]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这种技术很有价值，原因有以下几点。首先，它可以通过清除噪声维度和整合高度相关的维度来帮助我们清理数据。</p><p class="calibre_">第二，在提取出数据的低维代表后，我们就可以运用一系列并不太适用于高维数据的技术。我们可以在本书的很多地方看到运用这种技术的例子。</p><p class="calibre_">同时，它既可以帮助你建立更棒的模型，又会使你的模型更难理解。很容易理解诸如“工作年限每增加一年，平均工资会增加 1 万美元”这样的结论。但诸如“第三主成分每增加 0.1，平均工资就会增加 1 万美元”这样的结论就很难理解了。</p><div class="mbp_pagebreak" id="calibre_pb_75"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
