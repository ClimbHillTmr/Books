<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos549136" class="calibre_"><span class="calibre6"><span class="bold"> 18.3　反向传播 </span></span></p><p class="calibre_">通常情况下，我们是不会以手动方式建立神经网络的。部分原因在于，神经网络解决的是比较大型的问题，比如图象识别可能会用到数百或成千上万的神经元。还有一部分原因是，我们通常无法“通过推理得出”这些神经元的安排方式。</p><p class="calibre_">相反，我们会像往常一样使用数据用来训练神经网络。一个流行的训练算法是反向传播（backpropagation），它与前面介绍过的梯度下降法比较类似。</p><p class="calibre_">假如我们有一个训练集，其中含有输入向量和相应的目标输出向量。例如，前面 <tt class="calibre7">xor_network</tt> 例子中的输入向量为 <tt class="calibre7">[1, 0]</tt> ，对应的目标输出端向量为 <tt class="calibre7">[1]</tt> 。同时，假定我们的网络已经拥有一组权重，那么接下来，我们就需要使用以下算法来调整这些权重。</p><p class="calibre_">1. 在输入向量上运行 <tt class="calibre7">feed_forward</tt> ，从而得到网络所有神经元的输出。</p><p class="calibre_">2. 这样，每个输出神经元都会得到一个误差，即目标值与输出值之差。</p><p class="calibre_">3. 计算作为神经元权重的函数的误差的梯度，然后根据误差降低最快的方向调整权重。</p><p class="calibre_">4. 将这些输出误差反向传播给隐藏层以便计算相应误差。</p><p class="calibre_">5. 计算这些误差的梯度，并利用同样的方式调整隐藏层的权重。</p><p class="calibre_">一般情况下，这个算法需要在整个训练集上多次迭代，直到网络收敛为止：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def backpropagate(network, input_vector, targets):<br class="calibre12"/><br class="calibre12"/>    hidden_outputs, outputs = feed_forward(network, input_vector)<br class="calibre12"/><br class="calibre12"/>    # the output * (1 - output) is from the derivative of sigmoid<br class="calibre12"/>    output_deltas = [output * (1 - output) * (output - target)<br class="calibre12"/>                     for output, target in zip(outputs, targets)]<br class="calibre12"/><br class="calibre12"/>    # adjust weights for output layer, one neuron at a time<br class="calibre12"/>    for i, output_neuron in enumerate(network[-1]):<br class="calibre12"/>        # focus on the ith output layer neuron<br class="calibre12"/>        for j, hidden_output in enumerate(hidden_outputs + [1]):<br class="calibre12"/>            # adjust the jth weight based on both<br class="calibre12"/>            # this neuron's delta and its jth input<br class="calibre12"/>            output_neuron[j] -= output_deltas[i] * hidden_output<br class="calibre12"/><br class="calibre12"/>    # back-propagate errors to hidden layer<br class="calibre12"/>    hidden_deltas = [hidden_output * (1 - hidden_output) *<br class="calibre12"/>                      dot(output_deltas, [n[i] for n in output_layer])<br class="calibre12"/>                     for i, hidden_output in enumerate(hidden_outputs)]<br class="calibre12"/><br class="calibre12"/>    # adjust weights for hidden layer, one neuron at a time<br class="calibre12"/>    for i, hidden_neuron in enumerate(network[0]):<br class="calibre12"/>        for j, input in enumerate(input_vector + [1]):<br class="calibre12"/>            hidden_neuron[j] -= hidden_deltas[i] * input<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">实际上，以上代码所做的事情无异于显式写出与权重有关的均方误差，并应用第 8 章建立的 <tt class="calibre7">minimize_stochastic</tt> 函数。</p><p class="calibre_">就本例而言，显式写出梯度函数非常麻烦。如果你熟悉微积分和链式法则，那么这些数学细节对你来说还算简单，但是直接用文字表述的话（“误差函数对神经元 <span class="italic">i</span> 至神经元 <span class="italic">j</span> 输入上的权重求偏导数”）则相当无趣。</p><div class="mbp_pagebreak" id="calibre_pb_129"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
