<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos526128" class="calibre_"><span class="calibre6"><span class="bold"> 17.5　综合运用 </span></span></p><p class="calibre_">既然我们已经知道了这个算法的工作原理，下面就以更加通用的方式来实现这个算法。这意味着我们需要决定如何表示决策树。这里，我们将尽可能使用最轻量化的表示方法。我们将<span class="bold">树</span> 定义为下列情况之一：</p><div class="calibre_5"> </div><ul class="calibre_6"><li value="1" class="calibre_7"><p class="calibre_">
<tt class="calibre7">True</tt>
</p></li><li value="2" class="calibre_8"><p class="calibre_">
<tt class="calibre7">False</tt>
</p></li><li value="3" class="calibre_8"><p class="calibre_">元组 <tt class="calibre7">(attribute, subtree_dict)</tt>
</p></li></ul><p class="calibre_">这里的 <tt class="calibre7">True</tt> 代表一个叶节点，对于任何输入该节点都会返回 <tt class="calibre7">True</tt> ；<tt class="calibre7">False</tt> 也表示一个叶节点，但是对于任何输入该节点都会返回 <tt class="calibre7">False</tt> ；而元组则代表一个决策节点，对于任何输入，该节点都会根据 <tt class="calibre7">attribute</tt> 的值利用相应的子树对输入进行分类。</p><p class="calibre_">使用这种方法，我们的招聘决策树将表示如下：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>('level',<br class="calibre12"/> {'Junior': ('phd', {'no': True, 'yes': False}),<br class="calibre12"/>  'Mid': True,<br class="calibre12"/>  'Senior': ('tweets', {'no': False, 'yes': True})})<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">不过还有一个问题需要解决，即如何处理非预期的属性值和缺失属性值的情形。如果招聘决策树遇到应聘者的 <tt class="calibre7">level</tt> 属性值为“Intern”的情况，该如何处置呢？我们可以通过添加一个关键字 <tt class="calibre7">None</tt> 来处理这种情况，这时只要把预测结果设为最常见的标签即可。（当然，如果数据集中实际上含有 <tt class="calibre7">None</tt> 这个值的话，这将是一个糟糕的主意。）</p><p class="calibre_">给定了表示方法后，我们就可以对输入进行分类了，具体如下所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def classify(tree, input):<br class="calibre12"/>    """classify the input using the given decision tree"""<br class="calibre12"/><br class="calibre12"/>    # 如果这是一个叶节点，则返回其值<br class="calibre12"/>    if tree in [True, False]:<br class="calibre12"/>        return tree<br class="calibre12"/><br class="calibre12"/>    # 否则这个树就包含一个需要划分的属性<br class="calibre12"/>    # 和一个字典，字典的键是那个属性的值<br class="calibre12"/>    # 值是下一步需要考虑的子树<br class="calibre12"/>    attribute, subtree_dict = tree<br class="calibre12"/><br class="calibre12"/>    subtree_key = input.get(attribute)    # 如果输入的是缺失的属性，则返回None<br class="calibre12"/><br class="calibre12"/>    if subtree_key not in subtree_dict:   # 如果键没有子树<br class="calibre12"/>        subtree_key = None                # 则需要用到None子树<br class="calibre12"/><br class="calibre12"/>    subtree = subtree_dict[subtree_key]   # 选择恰当的子树<br class="calibre12"/>    return classify(subtree, input)       # 并用它来对输入分类<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">最后要做的就是利用训练数据建立决策树的具体表示形式：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def build_tree_id3(inputs, split_candidates=None):<br class="calibre12"/><br class="calibre12"/>    # 如果这是第一步<br class="calibre12"/>    # 第一次输入的所有的键就都是split candidates<br class="calibre12"/>    if split_candidates is None:<br class="calibre12"/>        split_candidates = inputs[0][0].keys()<br class="calibre12"/><br class="calibre12"/>    # 对输入里的True和False计数<br class="calibre12"/>    num_inputs = len(inputs)<br class="calibre12"/>    num_trues = len([label for item, label in inputs if label])<br class="calibre12"/>    num_falses = num_inputs - num_trues<br class="calibre12"/><br class="calibre12"/>    if num_trues == 0: return False     # 若没有True，则返回一个"False"叶节点<br class="calibre12"/>    if num_falses == 0: return True     # 若没有False，则返回一个"True"叶节点<br class="calibre12"/><br class="calibre12"/>    if not split_candidates:            # 若不再有split candidates<br class="calibre12"/>        return num_trues &gt;= num_falses  # 则返回多数叶节点<br class="calibre12"/><br class="calibre12"/>    # 否则在最好的属性上进行划分<br class="calibre12"/>    best_attribute = min(split_candidates,<br class="calibre12"/>                         key=partial(partition_entropy_by, inputs))<br class="calibre12"/><br class="calibre12"/>    partitions = partition_by(inputs, best_attribute)<br class="calibre12"/>    new_candidates = [a for a in split_candidates<br class="calibre12"/>                      if a != best_attribute]<br class="calibre12"/><br class="calibre12"/>    # 递归地创建子树<br class="calibre12"/>    subtrees = { attribute_value : build_tree_id3(subset, new_candidates)<br class="calibre12"/>                 for attribute_value, subset in partitions.iteritems() }<br class="calibre12"/><br class="calibre12"/>    subtrees[None] = num_trues &gt; num_falses      # 默认情况<br class="calibre12"/><br class="calibre12"/>    return (best_attribute, subtrees)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">在我们所建的树上，每一个叶节点要么由清一色的 <tt class="calibre7">True</tt> 输入组成，要不就是由清一色的 <tt class="calibre7">False</tt> 输入组成。这意味着，该决策树对于这个训练数据集的预测效果堪称完美。但我们也可以把它应用到训练集之外的新数据上面：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>tree = build_tree_id3(inputs)<br class="calibre12"/><br class="calibre12"/>classify(tree, { "level" : "Junior",<br class="calibre12"/>                 "lang" : "Java",<br class="calibre12"/>                 "tweets" : "yes",<br class="calibre12"/>                 "phd" : "no"} )        # True<br class="calibre12"/><br class="calibre12"/>classify(tree, { "level" : "Junior",<br class="calibre12"/>                 "lang" : "Java",<br class="calibre12"/>                 "tweets" : "yes",<br class="calibre12"/>                 "phd" : "yes"} )       # False<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">同时，也可以将它应用于具有缺失值或非预期值的数据：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>classify(tree, { "level" : "Intern" } ) # True<br class="calibre12"/>classify(tree, { "level" : "Senior" } ) # False<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><blockquote class="calibre_33"><img src="images/00100.jpg" class="calibre_10"/> 　由于我们的目的主要是演示<span class="bold">如何</span> 构建决策树，因此这里使用了整个数据集来建立决策树。与往常一样，如果现实中我们想创造一个优秀模型的话，就应该（收集更多的数据并且）将数据分成训练子集、验证子集和测试子集。</blockquote><div class="mbp_pagebreak" id="calibre_pb_123"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
