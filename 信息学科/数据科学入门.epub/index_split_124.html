<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos533326" class="calibre_"><span class="calibre6"><span class="bold"> 17.6　随机森林 </span></span></p><p class="calibre_">由于决策树与其训练数据的契合程度非常高，因此，它总是倾向于出现过拟合现象。为了避免出现这种情况，可以使用<span class="bold">随机森林</span> （random forest）技术。利用该技术，我们可以建立多个决策树，然后通过投票方式决定如何对输入进行分类：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def forest_classify(trees, input):<br class="calibre12"/>    votes = [classify(tree, input) for tree in trees]<br class="calibre12"/>    vote_counts = Counter(votes)<br class="calibre12"/>    return vote_counts.most_common(1)[0][0]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们知道，决策树的构建是一个确定性的过程，那么如何才能得到随机的决策树呢？</p><p class="calibre_">总的来说，这需要对数据进行 Bootstrap 抽样处理（这种抽样方法我们曾经在 15.6 节“题外话：Bootstrap”介绍过）。这种方法不是利用训练集合中的所有的输入数据来训练每棵决策树，而是利用 <tt class="calibre7">bootstrap_sample(inputs)</tt> 的取样结果来训练每棵决策树。因为每一棵决策树都是用不同的数据建立的，因此与其他决策树相比，每一棵都有其独特之处。（该方法的另一个好处是可以统一使用非抽样数据来测试每一棵决策树，这意味着如果你的模型效果评测方式设计得巧妙，完全可以将所有数据都用于训练集。）这种技术就是著名的 Bootstrap <span class="bold">集成法</span> （bootstrap aggregating），或者简称 bagging 方法。</p><p class="calibre_">随机性的另一个来源是在分类时不断变换选择最佳属性（<tt class="calibre7">best_attribute</tt> ）进行划分的方法。这里不是说选择全部的剩余属性进行划分，而是先从中随机选取一个子集，然后从中寻找最佳属性进行划分：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 如果已经存在了几个足够的划分候选项，就查看全部<br class="calibre12"/>if len(split_candidates) &lt;= self.num_split_candidates:<br class="calibre12"/>    sampled_split_candidates = split_candidates<br class="calibre12"/># 否则选取一个随机样本<br class="calibre12"/>else:<br class="calibre12"/>    sampled_split_candidates = random.sample(split_candidates,<br class="calibre12"/>                                             self.num_split_candidates)<br class="calibre12"/><br class="calibre12"/># 现在仅从这些候选项中选择最佳属性<br class="calibre12"/>best_attribute = min(sampled_split_candidates,<br class="calibre12"/>    key=partial(partition_entropy_by, inputs))<br class="calibre12"/><br class="calibre12"/>partitions = partition_by(inputs, best_attribute)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">上面的代码展示的是一种用途更广泛的技术，称为<span class="bold">集成学习</span> （ensemble learning），它能够将多个<span class="bold">较弱的</span> 模型（weak learner，通常是高偏差、低方差模型）组合成一个更加强大的模型。</p><p class="calibre_">随机森林是最为流行的一种集成方法，由其衍生的模型几乎到处可见。</p><div class="mbp_pagebreak" id="calibre_pb_124"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
