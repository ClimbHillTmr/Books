<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos296323" class="calibre_"><span class="calibre6"><span class="bold"> 9.3　网络抓取 </span></span></p><p class="calibre_">另一种获取数据的方法是从网页抓取数据。获取一个网页十分容易，但从网页上抓取有意义的结构化信息就不那么容易了。</p><p id="filepos296598" class="calibre_"><span class="calibre6"><span class="bold"> 9.3.1　HTML和解析方法 </span></span></p><p class="calibre_">网络上的页面是由 HTML 写成的，其中文本被（理想化地）标记为元素和它们的属性：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>&lt;html&gt;<br class="calibre12"/>  &lt;head&gt;<br class="calibre12"/>    &lt;title&gt;A web page&lt;/title&gt;<br class="calibre12"/>  &lt;/head&gt;<br class="calibre12"/>  &lt;body&gt;<br class="calibre12"/>    &lt;p id="author"&gt;Joel Grus&lt;/p&gt;<br class="calibre12"/>    &lt;p id="subject"&gt;Data Science&lt;/p&gt;<br class="calibre12"/>  &lt;/body&gt;<br class="calibre12"/>&lt;/html&gt;<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">在理想的情况下，所有的网页为我们方便地按语义标记，我们可以使用类似这样的规则来提取数据：找到 <tt class="calibre7">id</tt> 是 <tt class="calibre7">subject</tt> 的 <tt class="calibre7">&lt;p&gt;</tt> 元素并返回它所包含的文本。但在真实的世界中，HTML 并不总是具有很好的格式的，更不用说注解了。这意味着如果我们想搞清其含义，需要一些帮助。</p><p class="calibre_">为了从 HTML 里得到数据，我们需要使用 BeatifulSoup 库（<a href="http://www.crummy.com/software/BeautifulSoup/" class="calibre3">http://www.crummy.com/software/BeautifulSoup/</a> ），它对来自网页的多种元素建立了树结构，并提供了简单的接口来获取它们。本书写作时，最新的版本是 Beatiful Soup 4.3.2（<tt class="calibre7">pip install beautifulsoup4</tt> ），我们即将用到的就是这个版本。我们也会用到 requests 库（<tt class="calibre7">pip install requests</tt> ，<a href="http://docs.python-requests.org/en/latest/" class="calibre3">http://docs.python-requests.org/en/latest/</a> ），它与内置在 Python 中的其他方法相比，是一种发起 HTTP 请求的更好的方式。</p><p class="calibre_">Python 内置的 HTML 解析器是有点严格的，这意味着它并不总是能处理那些没有很好地格式化的 HTML。因此，我们需要使用另外一种解析器，它需要先安装：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>pip install html5lib<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">为了使用 Beatiful Soup，我们要把一些 HTML 传递给 <tt class="calibre7">BeautifulSoup()</tt> 函数。在我们的例子中，这些 HTML 是对 <tt class="calibre7">requests.get</tt> 进行调用的结果：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>from bs4 import BeautifulSoup<br class="calibre12"/>import requests<br class="calibre12"/>html = requests.get("http://www.example.com").text<br class="calibre12"/>soup = BeautifulSoup(html, 'html5lib')<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">完成这个步骤之后，我们可以用一些简单的方法得到完美的解析。</p><p class="calibre_">通常我们会处理一些 <tt class="calibre7">Tag</tt> 对象，它们对应于 HTML 页面结构的标签表示。</p><p class="calibre_">比如，找到你能用的第一个 <tt class="calibre7">&lt;p&gt;</tt> 标签（及其内容）：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>first_paragraph = soup.find('p')        # 或仅仅soup.p<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">可以对 <tt class="calibre7">Tag</tt> 使用它的 <tt class="calibre7">text</tt> 属性来得到文本内容：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>first_paragraph_text = soup.p.text<br class="calibre12"/>first_paragraph_words = soup.p.text.split()<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">另外可以把标签当作字典来提取其属性：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>first_paragraph_id = soup.p['id']        # 如果没有'id'则报出KeyError<br class="calibre12"/>first_paragraph_id2 = soup.p.get('id')   # 如果没有'id'则返回None<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">可以一次得到多个标签：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>all_paragraphs = soup.find_all('p')     # 或仅仅soup('p')<br class="calibre12"/>paragraphs_with_ids = [p for p in soup('p') if p.get('id')]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">通常你会想通过一个类（<tt class="calibre7">class</tt> ）来找到标签：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>important_paragraphs = soup('p', {'class' : 'important'})<br class="calibre12"/>important_paragraphs2 = soup('p', 'important')<br class="calibre12"/>important_paragraphs3 = [p for p in soup('p')<br class="calibre12"/>                         if 'important' in p.get('class', [])]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">此外，可以把这些方法组合起来运用更复杂的逻辑。比如，如果想找出包含在一个 <tt class="calibre7">&lt;div&gt;</tt> 元素中的每一个 <tt class="calibre7">&lt;span&gt;</tt> 元素，可以这么做：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 警告，将多次返回同一个span元素<br class="calibre12"/># 如果它位于多个div元素里<br class="calibre12"/># 如果是这种情况，要更谨慎一些<br class="calibre12"/>spans_inside_divs = [span<br class="calibre12"/>                     for div in soup('div')     # 对页面上的每个&lt;div&gt;<br class="calibre12"/>                     for span in div('span')]   # 找到其中的每一个&lt;span&gt;<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">仅仅上述几个特性就可以帮助我们做很多事。如果你需要做更复杂的事情（或仅仅是出于好奇），那就去查看文档吧。</p><p class="calibre_">当然，无论多重要的数据，通常也不会标记成 <tt class="calibre7">class="important"</tt> 。你需要仔细检查源 HTML，通过你选择的逻辑进行推理，并多考虑边界情况来确保数据的正确性。接下来我们看一个例子。</p><p id="filepos302689" class="calibre_"><span class="calibre6"><span class="bold"> 9.3.2　案例：关于数据的O'Reilly图书 </span></span></p><p class="calibre_">DataSciencester 的某位潜在投资者认为数据只会风靡一时。为了证明他是错的，你打算查看一下 O'Reilly 出版社这些年来总共出版过多少数据类的图书。通过对 O'Reilly 网站的挖掘，你发现它有许多有关数据图书（以及视频）的页面，每次 30 个条目的目录页面有这样的 URL：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>http://shop.oreilly.com/category/browse-subjects/data.do?sortby=publicationDate&amp;page=1<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们都不笨（而且也不想让自己的抓取器被封），所以在每次从网站抓取数据之前都该看一下这家网站是否有某种获取政策。查看以下页面：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>http://oreilly.com/terms/<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">看起来对这个项目没有明文禁止。但为了做一个守法的好公民，我们还应该查看一下 robots.txt 文件，看看一个网络抓取者要有怎样的行为规范。<a href="http://shop.oreilly.com/robots.txt" class="calibre3">http://shop.oreilly.com/robots.txt</a> 有以下重要内容：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>Crawl-delay: 30<br class="calibre12"/>Request-rate: 1/30<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">第一行告诉我们应该在两次请求之间等待 30 秒，第二行告诉我们每 30 秒只能请求一个页面。所以从根本上说这两行原则讲的是同一件事。（文件里还有一些内容说明有些目录页是不能抓取的，但是我们的 URL 不在其中，所以我们可以放心了。）</p><blockquote class="calibre_14"><img src="images/00100.jpg" class="calibre_10"/> 　O'Reilly 是有可能改变某些网站政策的，那样会打破本小节的所有逻辑。我会尽我所能预防这种情况的发生，当然，我对 O'Reilly 并没有太大的影响力。然而，如果你们每人都发动所有认识的人买一本这书的话……</blockquote><p class="calibre_">为了弄清该怎样提取数据，让我们下载其中一个页面，把它传给 Beatiful Soup：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/># 除非是写进书里，否则你没必要这样拆分一个url<br class="calibre12"/>url = "http://shop.oreilly.com/category/browse-subjects/" + \<br class="calibre12"/>      "data.do?sortby=publicationDate&amp;page=1"<br class="calibre12"/>soup = BeautifulSoup(requests.get(url).text, 'html5lib')<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">如果你查看页面的源代码（在浏览器中右键选择“查看源代码”或“查看网页源代码”，或其他最接近的选项），会看到每本书（或每部视频）都唯一地包含在一个表格单元格元素 <tt class="calibre7">&lt;td&gt;</tt> 中，它的类是 <tt class="calibre7">thumbtext</tt> 。下面的内容是某本书相关的 HTML（一个删减的版本）：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>&lt;td class="thumbtext"&gt;<br class="calibre12"/>  &lt;div class="thumbcontainer"&gt;<br class="calibre12"/>    &lt;div class="thumbdiv"&gt;<br class="calibre12"/>      &lt;a href="/product/9781118903407.do"&gt;<br class="calibre12"/>        &lt;img src="..."/&gt;<br class="calibre12"/>      &lt;/a&gt;<br class="calibre12"/>    &lt;/div&gt;<br class="calibre12"/>  &lt;/div&gt;<br class="calibre12"/>  &lt;div class="widthchange"&gt;<br class="calibre12"/>    &lt;div class="thumbheader"&gt;<br class="calibre12"/>      &lt;a href="/product/9781118903407.do"&gt;Getting a Big Data Job For Dummies&lt;/a&gt;<br class="calibre12"/>    &lt;/div&gt;<br class="calibre12"/>    &lt;div class="AuthorName"&gt;By Jason Williamson&lt;/div&gt;<br class="calibre12"/>    &lt;span class="directorydate"&gt;        December 2014     &lt;/span&gt;<br class="calibre12"/>    &lt;div style="clear:both;"&gt;<br class="calibre12"/>      &lt;div id="146350"&gt;<br class="calibre12"/>        &lt;span class="pricelabel"&gt;<br class="calibre12"/>                            Ebook:<br class="calibre12"/><br class="calibre12"/>                            &lt;span class="price"&gt;&amp;nbsp;$29.99&lt;/span&gt;<br class="calibre12"/>        &lt;/span&gt;<br class="calibre12"/>      &lt;/div&gt;<br class="calibre12"/>    &lt;/div&gt;<br class="calibre12"/>  &lt;/div&gt;<br class="calibre12"/>&lt;/td&gt;<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">良好的开端是找到所有的 <tt class="calibre7">td thumbtext</tt> 标签元素：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>tds = soup('td', 'thumbtext')<br class="calibre12"/>print len(tds)<br class="calibre12"/># 30<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">接下来我们要过滤掉视频。（那位潜在的投资者只对书感兴趣。）如果我们进一步地检查 HTML，会看到每个 <tt class="calibre7">td</tt> 会包含一个或更多个类为 <tt class="calibre7">pricelabel</tt> 的 <tt class="calibre7">span</tt> 元素，它的文本看起来像 <tt class="calibre7">Ebook:</tt> 或者 <tt class="calibre7">video:</tt> 或者 <tt class="calibre7">Print:</tt> 。看起来视频仅包含一个 <tt class="calibre7">pricelabel</tt> ，它的文本以 <tt class="calibre7">Video</tt> （在移除前导空格之后）开头。这意味着我们可以这样来检测视频：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def is_video(td):<br class="calibre12"/>    """it's a video if it has exactly one pricelabel, and if<br class="calibre12"/>    the stripped text inside that pricelabel starts with 'Video'"""<br class="calibre12"/>    pricelabels = td('span', 'pricelabel')<br class="calibre12"/>    return (len(pricelabels) == 1 and<br class="calibre12"/>            pricelabels[0].text.strip().startswith("Video"))<br class="calibre12"/><br class="calibre12"/>print len([td for td in tds if not is_video(td)])<br class="calibre12"/># 对我来说结果是21，你得到的结果可能会不同<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">现在我们已准备好要从 <tt class="calibre7">td</tt> 元素中提取数据了。看起来图书的标题是包含在 <tt class="calibre7">&lt;div class="thumbheader"&gt;</tt> 里的标签 <tt class="calibre7">&lt;a&gt;</tt> 中的文本：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>title = td.find("div", "thumbheader").a.text<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">作者（们）的名字在 <tt class="calibre7">AuthorName &lt;div&gt;</tt> 的文本里。它们由一个 <tt class="calibre7">By</tt> （我们打算去掉它）开头，由逗号分隔（我们打算把它们分隔开，然后去掉其中的空格）：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>author_name = td.find('div', 'AuthorName').text<br class="calibre12"/>authors = [x.strip() for x in re.sub("^By ", "", author_name).split(",")]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">ISBN 看起来是包含在 <tt class="calibre7">thumbheader &lt;div&gt;</tt> 中的链接里：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>isbn_link = td.find("div", "thumbheader").a.get("href")<br class="calibre12"/><br class="calibre12"/># re.match捕捉了括号中的正则表达式部分<br class="calibre12"/>isbn = re.match("/product/(.*)\.do", isbn_link).group(1)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">日期就是 <tt class="calibre7">&lt;span class="directorydate"&gt;</tt> 的内容：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>date = td.find("span", "directorydate").text.strip()<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">让我们把所有这些都放到一个函数里边：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def book_info(td):<br class="calibre12"/>    """given a BeautifulSoup &lt;td&gt; Tag representing a book,<br class="calibre12"/>    extract the book's details and return a dict"""<br class="calibre12"/><br class="calibre12"/>    title = td.find("div", "thumbheader").a.text<br class="calibre12"/>    by_author = td.find('div', 'AuthorName').text<br class="calibre12"/>    authors = [x.strip() for x in re.sub("^By ", "", by_author).split(",")]<br class="calibre12"/>    isbn_link = td.find("div", "thumbheader").a.get("href")<br class="calibre12"/>    isbn = re.match("/product/(.*)\.do", isbn_link).groups()[0]<br class="calibre12"/>    date = td.find("span", "directorydate").text.strip()<br class="calibre12"/><br class="calibre12"/>    return {<br class="calibre12"/>        "title" : title,<br class="calibre12"/>        "authors" : authors,<br class="calibre12"/>        "isbn" : isbn,<br class="calibre12"/>        "date" : date<br class="calibre12"/>    }<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">现在我们准备好进行抓取了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>from bs4 import BeautifulSoup<br class="calibre12"/>import requests<br class="calibre12"/>from time import sleep<br class="calibre12"/>base_url = "http://shop.oreilly.com/category/browse-subjects/" + \<br class="calibre12"/>           "data.do?sortby=publicationDate&amp;page="<br class="calibre12"/><br class="calibre12"/>books = []<br class="calibre12"/><br class="calibre12"/>NUM_PAGES = 31     # 这是写作本书时的值，现在有可能更多<br class="calibre12"/><br class="calibre12"/>for page_num in range(1, NUM_PAGES + 1):<br class="calibre12"/>    print "souping page", page_num, ",", len(books), " found so far"<br class="calibre12"/>    url = base_url + str(page_num)<br class="calibre12"/>    soup = BeautifulSoup(requests.get(url).text, 'html5lib')<br class="calibre12"/><br class="calibre12"/>    for td in soup('td', 'thumbtext'):<br class="calibre12"/>        if not is_video(td):<br class="calibre12"/>            books.append(book_info(td))<br class="calibre12"/><br class="calibre12"/># 现在做一个好公民，遵守robots.txt！<br class="calibre12"/>sleep(30)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><blockquote class="calibre_33"><img src="images/00100.jpg" class="calibre_10"/> 　像这样从 HTML 中提取数据更像是一种数据艺术而不是数据科学。除了上例之外，你还可以从 HTML 中实施查找图书、查找标题等不计其数的类似的逻辑行为。</blockquote><p class="calibre_">既然已经收集好了数据，现在就可以把每一年出版的图书数据绘制出来（如图 9-1）：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def get_year(book):<br class="calibre12"/>    """book["date"] looks like 'November 2014' so we need to<br class="calibre12"/>    split on the space and then take the second piece"""<br class="calibre12"/>    return int(book["date"].split()[1])<br class="calibre12"/><br class="calibre12"/># 2014是包含数据的最后一个完整的年份（我运行这段代码的时间）<br class="calibre12"/>year_counts = Counter(get_year(book) for book in books<br class="calibre12"/>                      if get_year(book) &lt;= 2014)<br class="calibre12"/><br class="calibre12"/>import matplotlib.pyplot as plt<br class="calibre12"/>years = sorted(year_counts)<br class="calibre12"/>book_counts = [year_counts[year] for year in years]<br class="calibre12"/>plt.plot(years, book_counts)<br class="calibre12"/>plt.ylabel("数据图书的数量")<br class="calibre12"/>plt.title("数据大发展！")<br class="calibre12"/>plt.show()<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_19"><img src="images/00070.jpg" class="calibre_21"/>
</p><p class="calibre_">
<span class="bold">图 9-1：每年数据图书的出版数量</span>
</p><p class="calibre_">不幸的是，那位潜在的投资者看到这张图后断言 2013 年是“数据时代的巅峰”。</p><div class="mbp_pagebreak" id="calibre_pb_66"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
