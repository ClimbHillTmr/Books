<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos490623" class="calibre_"><span class="calibre6"><span class="bold"> 16.2　Logistic函数 </span></span></p><p class="calibre_">对于逻辑回归来说，我们需要用到 Logistic 函数，其图像如图 16-3 所示：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def logistic(x):<br class="calibre12"/>    return 1.0 / (1 + math.exp(-x))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_19"><img src="images/00152.jpg" class="calibre_26"/>
</p><p class="calibre_">
<span class="bold">图 16-3：Logistic 函数</span>
</p><p class="calibre_">随着输入的数字变大且符号为正，它的输出就会越来越接近 1。随着输入的数字变大且符号为负，它的输出就会越来越接近 0。此外，这个函数还有一个非常好的属性，即其导数可以通过下列代码简单求出：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def logistic_prime(x):<br class="calibre12"/>    return logistic(x) * (1 - logistic(x))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这一点可以用于拟合模型：</p><p class="calibre_12"><img src="images/00038.jpg" class="calibre_120"/>
</p><p class="calibre_">其中，<span class="italic">f</span> 表示 <tt class="calibre7">logistic</tt> 函数。</p><p class="calibre_">回想一下，对于线性回归来说，我们是通过最小化误差的平方之和的方式来拟合模型，最终选出令得到这些观测数据的可能性最大的 <span class="italic">β</span> 。</p><p class="calibre_">但是，这里两者并不是等价的，所以我们直接使用梯度下降法来最大化似然。也就是说，我们需要计算似然函数及其梯度。</p><p class="calibre_">已知 <span class="italic">β</span> ，我们的模型指出每个 <span class="italic">y<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 等于 1 的概率为 <span class="italic">f</span> (<span class="italic">x<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub> β</span> )，等于 0 的概率为 1-<span class="italic">f</span> (<span class="italic">x<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub> β</span> )。</p><p class="calibre_">特别是，<span class="italic">y<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 的概率密度函数为：</p><p class="calibre_12"><img src="images/00040.jpg" class="calibre_121"/>
</p><p class="calibre_">如果 <span class="italic">y<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 为 0，则这等同于：</p><p class="calibre_12"><img src="images/00048.jpg" class="calibre_122"/>
</p><p class="calibre_">且如果 <span class="italic">y<sub class="calibre17"><small class="calibre9"><span class="calibre18"><span class="italic">i</span></span></small></sub>
</span> 为 1，则等同于：</p><p class="calibre_12"><img src="images/00073.jpg" class="calibre_123"/>
</p><p class="calibre_">事实表明，最大化对数似然要更加简单一些：</p><p class="calibre_12"><img src="images/00119.jpg" class="calibre_124"/>
</p><p class="calibre_">由于对数函数是单调递增函数，所以任何能够最大化对数似然函数的 <tt class="calibre7">beta</tt> 必然也能最大化似然函数，反之亦然。</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def logistic_log_likelihood_i(x_i, y_i, beta):<br class="calibre12"/>    if y_i == 1:<br class="calibre12"/>        return math.log(logistic(dot(x_i, beta)))<br class="calibre12"/>    else:<br class="calibre12"/>        return math.log(1 - logistic(dot(x_i, beta)))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">如果我们假设各个数据点之间相互独立，那么整体的似然就是各个似然之积。换句话说，整体的对数似然就是各个对数似然之和：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def logistic_log_likelihood(x, y, beta):<br class="calibre12"/>    return sum(logistic_log_likelihood_i(x_i, y_i, beta)<br class="calibre12"/>               for x_i, y_i in zip(x, y))<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">利用少许微积分知识，我们就能求出梯度了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def logistic_log_partial_ij(x_i, y_i, beta, j):<br class="calibre12"/>    """here i is the index of the data point,<br class="calibre12"/>    j the index of the derivative"""<br class="calibre12"/><br class="calibre12"/>    return (y_i - logistic(dot(x_i, beta))) * x_i[j]<br class="calibre12"/><br class="calibre12"/>def logistic_log_gradient_i(x_i, y_i, beta):<br class="calibre12"/>    """the gradient of the log likelihood<br class="calibre12"/>    corresponding to the ith data point"""<br class="calibre12"/><br class="calibre12"/>    return [logistic_log_partial_ij(x_i, y_i, beta, j)<br class="calibre12"/>            for j, _ in enumerate(beta)]<br class="calibre12"/><br class="calibre12"/>def logistic_log_gradient(x, y, beta):<br class="calibre12"/>    return reduce(vector_add,<br class="calibre12"/>                  [logistic_log_gradient_i(x_i, y_i, beta)<br class="calibre12"/>                   for x_i, y_i in zip(x,y)])<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">好了，到此为止我们已经万事俱备了。</p><div class="mbp_pagebreak" id="calibre_pb_113"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
