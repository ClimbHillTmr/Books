<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos602248" class="calibre_"><span class="calibre6"><span class="bold"> 20.2　n-grams模型 </span></span></p><p class="calibre_">DataSciencester 负责搜索引擎营销的副总突发奇想，要创建数以千计的数据科学方面的 Web 页面，以便人们在搜索与数据科学有关的词语时，我们的网站能够在搜索结果中的排名更加靠前。（你试图向他解释，由于搜索引擎的算法已经足够聪明了，所以这种做法很难奏效，但是他根本就不听这一套。）</p><p class="calibre_">当然，他既不想亲自编写数以千计的网页，也不打算雇用一批“水军”来做这件事情。相反，他向你咨询是否可以通过编程方式来生成这些网页。为此，我们需要寻找某种方法来对语言进行建模。</p><p class="calibre_">这种方法当然是有的，比如首先搜集一批文档，然后利用统计方法得到一个语言模型。在我们的例子中，我们将从 Mike Loukides 的文章“什么是数据科学？”（<a href="https://www.oreilly.com/ideas/what-is-data-science" class="calibre3">https://www.oreilly.com/ideas/what-is-data-science</a> ）着手。</p><p class="calibre_">就像在第 9 章中所做的那样，我们将使用一些 Web 请求命令（<tt class="calibre7">requests</tt> ）和 <tt class="calibre7">BeautifulSoup</tt> 来检索数据。不过，这里有几个问题需要引起我们的注意。</p><p class="calibre_">第一个问题是，文本中的单引号实际上就是 Unicode 字符 <tt class="calibre7">u"\u2019"</tt> 。我们可以创建一个辅助函数，用正常的单引号来取代它们：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def fix_unicode(text):<br class="calibre12"/>    return text.replace(u"\u2019", "'")<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">第二个问题是，在获得了网页的文本之后，我们需要把它做成一个由单词和句号组成的序列（这样我们就可以知道句子的结尾在哪里）。为此，我们可以借助于 <tt class="calibre7">re.findall()</tt> 函数来完成：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>from bs4 import BeautifulSoup<br class="calibre12"/>import requests<br class="calibre12"/>url = "http://radar.oreilly.com/2010/06/what-is-data-science.html"<br class="calibre12"/>html = requests.get(url).text<br class="calibre12"/>soup = BeautifulSoup(html, 'html5lib')<br class="calibre12"/><br class="calibre12"/>content = soup.find("div", "entry-content")   # 找到entry-content div<br class="calibre12"/>regex = r"[\w']+|[\.]"                        # 匹配一个单词或一个句点<br class="calibre12"/><br class="calibre12"/>document = []<br class="calibre12"/><br class="calibre12"/>for paragraph in content("p"):<br class="calibre12"/>    words = re.findall(regex, fix_unicode(paragraph.text))<br class="calibre12"/>    document.extend(words)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">当然，我们可以（也应该）进一步清理这些数据。文档中依然存在一些多余的文字（例如，第一个字“Section”就多余），同时，我们是利用句点来断句的（例如，遇到“Web 2.0”就会出问题）。此外，文档中还散布了一些标题和列表。尽管如此，这个文档已经可以凑合着用了。</p><p class="calibre_">将文本做成了单词序列之后，我们就可以通过以下方式对语言进行建模了：给定某个起始单词（比如“book”），我们可以找出源文档中所有在它后面出现过的那些单词（这里是“isn't”“a”“shows”“demonstrates”和“teaches”）。我们从中随机选择一个来作为下一个单词，然后重复这个过程，直到我们遇到一个句点为止，因为句点就意味着句子的结束。我们将这个模型称之为<span class="bold">二元模型</span> （bigram model），因为这完全是由原始数据中 2 个词（一个词对）同时出现的频率决定的。</p><p class="calibre_">那么起始单词呢？实际上，我们只要从句点<span class="bold">后面</span> 的单词中随机选择就行了。首先，让我们预先计算出可能的单词语次转变。回想一下，对于 zip 来说，只要输入中有一个已经处理完毕，它就会停下来，因此，我们可以利用 <tt class="calibre7">zip(document, document[1:])</tt> 求出文档中有多少对连续元素：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>bigrams = zip(document, document[1:])<br class="calibre12"/>transitions = defaultdict(list)<br class="calibre12"/>for prev, current in bigrams:<br class="calibre12"/>    transitions[prev].append(current)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">下面我们就可以生成句子了：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def generate_using_bigrams():<br class="calibre12"/>    current = "."   # 这意味着下一个单词是一个新句子的开头<br class="calibre12"/>    result = []<br class="calibre12"/>    while True:<br class="calibre12"/>        next_word_candidates = transitions[current]    # 双连词 (current, _)<br class="calibre12"/>        current = random.choice(next_word_candidates)  # 随机选择一个<br class="calibre12"/>        result.append(current)                         # 将其附加到结果中<br class="calibre12"/>        if current == ".": return " ".join(result)     # 如果是"."，就完成了<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">它产生的那些句子都是些无意义的数据，不过你可以把这些句子放到网站上，以让网站看起来更能与数据科学挂钩。举例来说：</p><blockquote class="calibre_14">If you may know which are you want to data sort the data feeds web friend someone on trending topics as the data in Hadoop is the data science requires a book demonstrates why visualizations are but we do massive correlations across many commercial disk drives in Python language and creates more tractable form making connections then use and uses it to solve a data.</blockquote><blockquote class="calibre_148">——Bigram Model</blockquote><p class="calibre_">如果我们使用<span class="bold">三元模型</span> （trigrams）的话，就能够降低这些句子无意义的程度。所谓三元模型，就是使用三个连续的词得到的模型。（更一般地讲，你还可以考虑由 <span class="italic">n</span> 个连续的单词得到的 <span class="italic">n</span> -grams 模型，不过对于我们来说，由三个词组成的就足够了。）现在，这种语次转变将取决于<span class="bold">前两个</span> 单词：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>trigrams = zip(document, document[1:], document[2:])<br class="calibre12"/>trigram_transitions = defaultdict(list)<br class="calibre12"/>starts = []<br class="calibre12"/><br class="calibre12"/>for prev, current, next in trigrams:<br class="calibre12"/><br class="calibre12"/>    if prev == ".":              # 如果前一个"单词"是个句点<br class="calibre12"/>        starts.append(current)   # 那么这就是一个起始单词<br class="calibre12"/><br class="calibre12"/>    trigram_transitions[(prev, current)].append(next)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">需要注意的是，现在我们必须将这些起始词单独记录下来。我们可以使用几乎相同的方法来生成句子：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def generate_using_trigrams():<br class="calibre12"/>    current = random.choice(starts)   # 随机选择一个起始单词<br class="calibre12"/>    prev = "."                            # 前面加一个句点'.'<br class="calibre12"/>    result = [current]<br class="calibre12"/>    while True:<br class="calibre12"/>        next_word_candidates = trigram_transitions[(prev, current)]<br class="calibre12"/>        next_word = random.choice(next_word_candidates)<br class="calibre12"/>        prev, current = current, next_word<br class="calibre12"/>        result.append(current)<br class="calibre12"/><br class="calibre12"/>        if current == ".":<br class="calibre12"/>            return " ".join(result)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">这次得到的句子看起来要好一些：</p><blockquote class="calibre_14">In hindsight MapReduce seems like an epidemic and if so does that give us new insights into how economies work That's not a question we could even have asked a few years there has been instrumented.</blockquote><blockquote class="calibre_148">——Trigram Model</blockquote><p class="calibre_">当然，它们之所以看起来更好一些，是因为生成过程的每一步中，所面临的选择要更少一些，甚至有时候只有一种选择。这就意味着生成的句子（或至少是长短语）经常跟原始数据中的一字不差。更多的数据会有所帮助；此外，如果从多篇数据科学方面的文章中收集 <span class="italic">n</span> -grams，收到的成效会更好。</p><div class="mbp_pagebreak" id="calibre_pb_142"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
