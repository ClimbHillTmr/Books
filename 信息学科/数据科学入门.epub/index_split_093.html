<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>数据科学入门</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles1.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos429039" class="calibre_"><span class="calibre6"><span class="bold"> 13.3　算法的实现 </span></span></p><p class="calibre_">到目前为止，我们已经学习了构建垃圾邮件分类器所需的各方面的知识。下面，我们首先建立一个简单的函数，来将邮件解析为不同的单词。首先要把各个邮件文本转换为小写形式，然后使用 <tt class="calibre7">re.findall()</tt> 提取由字母、数字和撇号组成的“单词”，最后使用 <tt class="calibre7">set()</tt> 函数获得不同的单词：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def tokenize(message):<br class="calibre12"/>    message = message.lower()                            # 转换为小写<br class="calibre12"/>    all_words = re.findall("[a-z0-9']+", message)        # 提取单词<br class="calibre12"/>    return set(all_words)                                # 移除副本<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">我们的第二个函数用来计算单词出现在已做标记的邮件训练集中的次数。该函数将返回一个字典，其键为单词，其值为列表，该列表包含两个元素 <tt class="calibre7">[spam_count, non_spam_count]</tt> ，分别表示该单词出现在垃圾邮件和非垃圾邮件中的次数。</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def count_words(training_set):<br class="calibre12"/>    """training set consists of pairs (message, is_spam)"""<br class="calibre12"/>    counts = defaultdict(lambda: [0, 0])<br class="calibre12"/>    for message, is_spam in training_set:<br class="calibre12"/>        for word in tokenize(message):<br class="calibre12"/>            counts[word][0 if is_spam else 1] += 1<br class="calibre12"/>    return counts<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">接下来，我们利用前面讲过的平滑技术将这些计数转换为估计概率。函数将返回一个列表，列表元素包含三方面的内容，分别是各个单词、该单词出现在垃圾邮件中的概率以及该单词出现在非垃圾邮件中的概率：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def word_probabilities(counts, total_spams, total_non_spams, k=0.5):<br class="calibre12"/>    """turn the word_counts into a list of triplets<br class="calibre12"/>    w, p(w | spam) and p(w | ~spam)"""<br class="calibre12"/>    return [(w,<br class="calibre12"/>             (spam + k) / (total_spams + 2 * k),<br class="calibre12"/>             (non_spam + k) / (total_non_spams + 2 * k))<br class="calibre12"/>             for w, (spam, non_spam) in counts.iteritems()]<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">最后要做的事情是利用这些单词的概率（以及朴素贝叶斯假设）给邮件赋予概率：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>def spam_probability(word_probs, message):<br class="calibre12"/>    message_words = tokenize(message)<br class="calibre12"/>    log_prob_if_spam = log_prob_if_not_spam = 0.0<br class="calibre12"/><br class="calibre12"/>    # 迭代词汇表中的每一个单词<br class="calibre12"/>    for word, prob_if_spam, prob_if_not_spam in word_probs:<br class="calibre12"/><br class="calibre12"/>        # 如果*word*出现在了邮件中<br class="calibre12"/>        # 则增加看到它的对数概率<br class="calibre12"/>        if word in message_words:<br class="calibre12"/>            log_prob_if_spam += math.log(prob_if_spam)<br class="calibre12"/>            log_prob_if_not_spam += math.log(prob_if_not_spam)<br class="calibre12"/><br class="calibre12"/>        # 如果*word*没有出现在邮件中<br class="calibre12"/>        # 则增加看不到它的对数概率<br class="calibre12"/>        # 也就是log(1 - 看到它的概率)<br class="calibre12"/>        else:<br class="calibre12"/>            log_prob_if_spam += math.log(1.0 - prob_if_spam)<br class="calibre12"/>            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)<br class="calibre12"/><br class="calibre12"/>    prob_if_spam = math.exp(log_prob_if_spam)<br class="calibre12"/>    prob_if_not_spam = math.exp(log_prob_if_not_spam)<br class="calibre12"/>    return prob_if_spam / (prob_if_spam + prob_if_not_spam)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><p class="calibre_15">将上面的代码结合起来，就得到了我们的朴素贝叶斯分类器：</p><blockquote class="calibre_14"><span class="calibre11"><tt class="calibre7"><br class="calibre12"/>class NaiveBayesClassifier:<br class="calibre12"/><br class="calibre12"/>    def __init__(self, k=0.5):<br class="calibre12"/>        self.k = k<br class="calibre12"/>        self.word_probs = []<br class="calibre12"/><br class="calibre12"/>    def train(self, training_set):<br class="calibre12"/><br class="calibre12"/>        # 对垃圾邮件和非垃圾邮件计数<br class="calibre12"/>        num_spams = len([is_spam<br class="calibre12"/>                         for message, is_spam in training_set<br class="calibre12"/>                         if is_spam])<br class="calibre12"/>        num_non_spams = len(training_set) - num_spams<br class="calibre12"/><br class="calibre12"/>        # 通过"pipeline"运行训练数据<br class="calibre12"/>        word_counts = count_words(training_set)<br class="calibre12"/>        self.word_probs = word_probabilities(word_counts,<br class="calibre12"/>                                             num_spams,<br class="calibre12"/>                                             num_non_spams,<br class="calibre12"/>                                             self.k)<br class="calibre12"/><br class="calibre12"/>    def classify(self, message):<br class="calibre12"/>        return spam_probability(self.word_probs, message)<br class="calibre12"/><br class="calibre12"/><br class="calibre12"/><br class="calibre12"/></tt></span></blockquote><div class="mbp_pagebreak" id="calibre_pb_93"></div>
<div class="calibre5">本书由「<a href="https://epubw.com" class="calibre3">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="calibre3">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body></html>
