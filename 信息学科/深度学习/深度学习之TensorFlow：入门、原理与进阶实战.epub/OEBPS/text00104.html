<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>未知</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h3 class="p">12.8　SRGAN——适用于超分辨率重建的GAN</h3>
<p class="ziti3">SRGAN属于GAN理论在超分辨率重建（SR）方面的应用。在学习SRGAN之前有必要先了解一下SR领域的相关技术。</p>
<h4 class="p3">12.8.1　超分辨率技术</h4>
<p class="ziti4">
<span class="yanse">1．SR（超分辨率重建）技术介绍</span>
</p>
<p class="ziti3">SR（Super-Resolution，超分辨率）技术，是指从观测到的低分辨率图像重建出相应的高分辨率图像，在监控设备、卫星图像和医学影像等领域都有重要的应用价值，该技术也可应用于马赛克图片的恢复应用场景中。</p>
<p class="ziti3">SR可分为两类：从多张低分辨率图像重建出高分辨率图像，和从单张低分辨率图像重建出高分辨率图像。基于深度学习的SR，主要是基于单张低分辨率的重建方法，即Single Image Super-Resolution（SISR）。</p>
<p class="ziti3">SISR是一个逆问题。对于一个低分辨率图像，可能存在许多不同的高分辨率图像与之对应，为了让逆向图片的结果更接近真实图片，则需要让模型在一定约束下，指定某个领域中来进行可逆训练，而这个约束，就是指现有的低分辨率像素的色度信息与位置信息。为了能让模型更好地学习并利用这个信息，基于深度学习的SR通过神经网络直接通过优化低分辨率图像到高分辨率图像的损失函数loss来进行端到端训练，以实现超分辨率重建功能。</p>
<p class="ziti4">
<span class="yanse">2．深度学习中的SR方法</span>
</p>
<p class="ziti3">在GAN出现之前，先是以SRCNN、DRCN为主的SR方法。该方法的大体思想是将低分辨率像素先扩展到高分辨率的像素大小，然后通过卷积方式训练网络，优化其与真实高分辨率图片的loss，最终形成模型。并且在这一方法上也总结了不少经验参数，如在SRCNN中，使用3层步长为1的同卷积，分别为（9×9的64输出、1×1的32输出、5×5的3输出）效果会更好。</p>
<p class="ziti3">后来出现了另一种比较高效的方法ESPCN（实时的基于卷积神经网络的图像超分辨率方法）。ESPCN的核心概念是亚像素卷积层（sub-pixel convolutional layer），即先在原有的低像素图片上做卷积操作，最终输出一个含有多feature map的结果，保证总像素点与高分辨率的像素点总和是一致的，然后将多张低分辨率图片合并成一张高分辨率图片。例如，假设需要将低分辨率图片的像素扩大2倍（从128×128扩大到256×256），就直接将其进行卷积操作，最终输出放大倍数的平方（2×2）个feature map，即[batch_size，W，H，4]（如果是RGB彩色图片就会是[batch_size，W，H，12]）。以灰度图为例，将4个图片中的第一个像素取出成为重构图中的4个像素，依此类推，在重构图中的每个2×2区域都是由这4幅图对应位置的像素组成，最终形成形状为[batch_size，2×W，2×H，1]大小的高分辨率图像。这个变换被称为sub-pixel convolution，如图12-12所示。</p>
<div class="pic"><img alt="" src="Image00275.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-12　ESPCN图例</p>
<p class="ziti3">sub-pixel convolution的方法只在最后一层进行图像低分辩率到高分辨率的大小变换，保证了前面的卷积运算均在低分辨率图像上进行，得到了更高的运算效率。</p>
<p class="ziti3">另外，基于视频图像的SR方法还有VESPCN，这里不再展开介绍，有兴趣的读者可以自行学习。</p>
<p class="ziti4">
<span class="yanse">3．TensorFlow中的图片变换函数</span>
</p>
<p class="ziti3">在TensorFlow中变化分辨率的函数主要是tf.image.resize_images，其具体原型如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">def resize_images(images,
                  size,
                  method=ResizeMethod.BILINEAR,
                  align_corners=False):</pre>
<hr class="calibre6"/>
<p class="ziti3">前两个参数分别是输入的图片及要变化的尺寸，图片的形状为[batch，height，width，channels]或[height，width，channels]均可。第3个参数method的取值如下。</p>
<p class="ziti4">·ResizeMethod.BILINEAR：表示使用双线性插值算法变化图片。</p>
<p class="ziti4">·ResizeMethod.NEAREST_NEIGHBOR：表示使用邻近值插值算法变化图片。</p>
<p class="ziti4">·ResizeMethod.BICUBIC：表示使用双立方插值算法变化图片。</p>
<p class="ziti4">·ResizeMethod.AREA：表示使用面积插值算法变化图片。</p>
<p class="ziti3">具体算法这里不展开介绍，后面会通过实例演示其各个算法的效果。</p>
<p class="ziti3">还可以直接使用内部函数来做类似的处理，例如：</p>
<hr class="calibre6"/>
<pre class="ziti5">tf.image.resize_bicubic(images,size,align_corners=None,name=None)
tf.image.resize_nearest_neighbor(images,size,align_corners=None,name=
None)
tf.image.resize_bilinear(images, size, align_corners=None, name=None)</pre>
<hr class="calibre6"/>
<p class="ziti3">与前面不同的是，images的格式只支持一种[batch，height，width，channels]。</p>
<p class="ziti4">
<span class="yanse">4．TensorFlow的图像变化函数汇总</span>
</p>
<p class="ziti3">TensorFlow的图像变化函数如表12-1所示。</p>
<p class="middle-img">表12-1　图像变化函数汇总</p>
<div class="pic"><img alt="" src="Image00276.jpg" class="calibre4"/>
</div>
<h4 class="p3">12.8.2　实例93：ESPCN实现MNIST数据集的超分辨率重建</h4>
<p class="ziti3">本实例主要通过对MNIST数据集实现超分辨率的重建，来示范TensorFlow中相关图片变化的函数用法和效果，以及ESPCN的网络结构。该实例共分为以下几步骤。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">通过使用ESPCN网络，在MNIST数据集上将低分辨率图片复原成高分辨率图片，并与其他复原函数的生成结果进行比较。</p>
<p class="ziti4">
<span class="yanse">1．引入头文件，构建低分辨率样本</span>
</p>
<p class="ziti3">在头文件部分导入slim库，使用resize_bicubic来构建缩小4倍的低分辨率样本，将28×28的像素变为14×14（长、宽个缩小2倍）。</p>
<p class="ziti3">代码12-6　mnistEspcn</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import tensorflow as tf
02 import matplotlib.pyplot as plt
03 import numpy as np
04 import tensorflow.contrib.slim as slim
05 from tensorflow.examples.tutorials.mnist import input_data
06 mnist = input_data.read_data_sets("/data/", one_hot=True)
07 
08 batch_size = 30    # 获取样本的批次大小
09 n_input = 784      # MNIST data 输入(img shape: 28*28)
10 n_classes = 10     # MNIST 列别 (0-9 ，一共10类)
11 
12 # 待输入的样本图片
13 x = tf.placeholder("float", [None, n_input])
14 img = tf.reshape(x,[-1,28,28,1])
15 # 缩小image
16 x_small = tf.image.resize_bicubic(img, (14, 14)) # 缩小一半</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．通过TensorFlow函数实现超分辨率</span>
</p>
<p class="ziti3">分别使用bicubic、nearest_neighbor和bilinear方法将分辨率还原，为了后续比较效果。</p>
<p class="ziti3">代码12-6　mnistEspcn（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">17 x_bicubic = tf.image.resize_bicubic(x_small, (28, 28))#双立方插值算法变化
18 x_nearest = tf.image.resize_nearest_neighbor(x_small, (28, 28))
19 x_bilin = tf.image.resize_bilinear(x_small, (28, 28))</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">3．建立ESPCN网络结构</span>
</p>
<p class="ziti3">建立一个简单的三层卷积网络：第1层使用5×5的卷积核，输出64通道的图片，slim卷积函数中使用的是默认激活函数Relu；第2层使用3×3的卷积核，输出的是32通道；最后一层使用3×3卷积核，生成4通道图片。这个4通道需要和恢复超分辨率缩放范围对应，4代表长、宽各放大2倍。接着使用tf.depth_to_space函数，将多张图片合并成一张图片。</p>
<p class="ziti3">tf.depth_to_space函数的意思是将深度数据按照块的模式展开重新排列，第一个输入是原始数据，第二个输入是块的尺寸，输入2则代表尺寸为2×2的块。而深度就是生成图片的通道数，即将每个通道对应的像素值填充到指定大小的块中。</p>
<p class="ziti3">代码12-6　mnistEspcn（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">20 #ESPCN网络结构
21 net = slim.conv2d(x_small, 64, 5)
22 net =slim.conv2d(net, 32, 3)
23 net = slim.conv2d(net, 4, 3)
24 net = tf.depth_to_space(net,2)
25 print("net.shape",net.shape)</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">4．构建loss及优化器</span>
</p>
<p class="ziti3">将图片重新调整形状（reshape）为（batch_size，784）的形状，通过平方差来计算loss，设定学习率为0.01，通过AdamOptimizer进行优化。</p>
<p class="ziti3">代码12-6　mnistEspcn（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">26 y_pred = tf.reshape(net,[-1,784])
27 
28 cost = tf.reduce_mean(tf.pow(x - y_pred, 2))
29 optimizer = tf.train.AdamOptimizer(0.01 ).minimize(cost)</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">5．建立session，运行</span>
</p>
<p class="ziti3">令数据即循环100次。启动session进行迭代训练。</p>
<p class="ziti3">代码12-6　mnistEspcn（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">30 training_epochs =100
31 display_step =20
32 
33 with tf.Session() as sess:
34     sess.run(tf.global_variables_initializer())
35     total_batch = int(mnist.train.num_examples/batch_size)
36     # 启动循环开始训练
37     for epoch in range(training_epochs):
38         # 遍历全部数据集
39         for i in range(total_batch):
40             batch_xs, batch_ys = mnist.train.next_batch(batch_size)  
41             _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs})
42         # 显示训练中的详细信息
43         if epoch % display_step == 0:
44             print("Epoch:", '%04d' % (epoch+1),
45                   "cost=", "{:.9f}".format(c))
46 
47     print("完成!")</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">6．图示结果</span>
</p>
<p class="ziti3">为了比较效果，将原始图片、低分辨率图片、各种算法的变化图片及模型恢复的图片一起显示出来。代码如下：</p>
<p class="ziti3">代码12-6　mnistEspcn（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">48 show_num = 10
49     encode_s,encode_b,encode_n ,encode_bi,y_predv= sess.run(
50         [x_small,x_bicubic,x_nearest,x_bilin,y_pred], feed_dict={x: 
        mnist.test.images[:show_num]})
51     
52     f, a = plt.subplots(6, 10, figsize=(10, 6))
53     for i in range(show_num):
54         a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))
55         a[1][i].imshow(np.reshape(encode_s[i], (14, 14)))
56         a[2][i].imshow(np.reshape(encode_b[i], (28, 28)))
57         a[3][i].imshow(np.reshape(encode_n[i], (28, 28)))
58         a[4][i].imshow(np.reshape(encode_bi[i], (28, 28)))
59         a[5][i].imshow(np.reshape(y_predv[i], (28, 28)))
60     plt.show()</pre>
<hr class="calibre6"/>
<p class="ziti3">运行代码，显示图片如图12-13所示。</p>
<div class="pic"><img alt="" src="Image00277.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-13　ESPCN实例MNIST结果</p>
<p class="ziti3">最后一行是模型恢复的数据，可以看到，清晰度完全超过前面几行。Bicubic和bilinear效果还可以，nearest_neighbor是最差的。</p>
<h4 class="p3">12.8.3　实例94：ESPCN实现flowers数据集的超分辨率重建</h4>
<p class="ziti3">前面的MNIST数据集轻巧方便，非常适合对模型的演示与理解。下面学习对彩色图片进行超分辨率的重建。彩色图片与MNIST样本不同的地方主要是，图片变为了3通道，并且像素点更多，而MNIST像素点更稀疏。所以应用在训练模型上，会有一些细节进行调节。通过对本例的学习，读者可以掌握对全彩色图片的一些处理技巧，这也是本例的主要意义。</p>
<p class="ziti3">本例主要实现对flowers数据集的图片处理。flowers数据集在第11章中的slim部分介绍过。本例同样还是使用slim模块进行数据的操作。另外flowers是尺寸不一的数据样本，所以在本例中也可以借鉴统一尺寸处理的方法。同样，本例还会示范TensorFlow中相关图片变化的函数用法和效果，以及ESPCN的网络结构。该实例共分为以下几步。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">通过使用ESPCN网络，在flower数据集上将低分辨率图片复原成高分辨率图片并与其他复原函数的生成结果进行比较。</p>
<p class="ziti4">
<span class="yanse">1．引入头文件，创建样本数据源</span>
</p>
<p class="ziti3">同样使用slim，这次使用的数据源是flowers，所以将该代码文件建立到models下面的slim下，然后就可以其引入flowers数据集了（这一步不熟悉的读者可以参考第11章的slim数据集部分）。</p>
<p class="ziti3">指定TFRecord文件夹，创建数据集。代码如下：</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import tensorflow as tf
02 from datasets import flowers
03 import numpy as np
04 import matplotlib.pyplot as plt
05 
06 slim = tf.contrib.slim
07 
08 height = width = 200
09 batch_size = 4
10 DATA_DIR="D:/own/python/flower_photosos"
11 
12 #选择数据集validation
13 dataset = flowers.get_split('validation', DATA_DIR)
14 #创建一个provider
15 provider = slim.dataset_data_provider.DatasetDataProvider(dataset,
num_readers = 2)
16 #通过provider的get拿到内容
17 [image, label] = provider.get(['image', 'label'])
18 print(image.shape)</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．获取批次样本并通过TensorFlow函数实现超分辨率</span>
</p>
<p class="ziti3">通过resize_image_with_crop_or_pad函数统一样本大小，大的剪掉，不够的加0填充。使用tf.train.batch函数获得指定批次数据images和labels。</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">19 # 剪辑图片为统一大小 
20 distorted_image = tf.image.resize_image_with_crop_or_pad(image, 
height, width) #剪辑尺寸，不够填充 
21 ################################################
22 images, labels = tf.train.batch([distorted_image, label], batch_size= 
batch_size)
23 print(images.shape)
24 
25 x_smalls = tf.image.resize_images(images, (np.int32(height/2), np.
int32(width/2)))#  尺寸变为原来的1/4
26 x_smalls2 = x_smalls/255.0
27 #还原
28 x_nearests = tf.image.resize_images(x_smalls, (height, width),tf.
image.ResizeMethod.NEAREST_NEIGHBOR)
29 x_bilins = tf.image.resize_images(x_smalls, (height, width),tf.
image.ResizeMethod.BILINEAR)
30 x_bicubics = tf.image.resize_images(x_smalls, (height, width),tf.
image.ResizeMethod.BICUBIC)</pre>
<hr class="calibre6"/>
<p class="ziti3">先通过resize_images创建一个低分辩率图片x_smalls，然后将x_smalls通过不同算法的变化，生成对应的高分辨率图片。</p>
<p class="ziti4">
<span class="yanse">3．建立ESPCN网络结构</span>
</p>
<p class="ziti3">网络结构与上例一样，不同的是输入的图片做了归一化处理，统一除以255，使其变为0～1之间的数。最后一个卷积成输出的为12通道，代表2×2的缩放比例，一共3个通道，所以再乘以3。另外，各层均使用了Tanh函数，最后一层没有使用激活函数。</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">31 net = slim.conv2d(x_smalls2, 64, 5,activation_fn = tf.nn.tanh)
32 net =slim.conv2d(net, 32, 3,activation_fn = tf.nn.tanh)
33 net = slim.conv2d(net, 12, 3,activation_fn = None)#2*2*3
34 y_predt = tf.depth_to_space(net,2)
35 
36 y_pred = y_predt*255.0
37 y_pred = tf.maximum(y_pred,0)
38 y_pred = tf.minimum(y_pred,255)
39 
40 dbatch=tf.concat([tf.cast(images,tf.float32),y_pred],0)</pre>
<hr class="calibre6"/>
<p class="ziti3">y_pred是由y_predt转化而来的，通过tf.maximum 与tf.minimum函数将内部的值都变为 0～255之间的数字。y_predt会参与损失值的计算。</p>
<p class="ziti3">dbatch是将生成的y_pred与images按照批次的维度合并起来，该张量是为了后面进行图片质量评估使用的。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 上面例子中y_pred进行了最大值和最小值的规整处理，这是个很常用的技巧。如果不处理，那么生成的图片会在显示时看到有亮点，使图片显得不清晰。读者可以试着将这段代码去掉，看看效果。</p>
<p class="ziti4">
<span class="yanse">4．构建loss及优化器</span>
</p>
<p class="ziti3">对于全彩色训练的学习率设定还是需要非常小心的，在这里设置为0.000001，让其缓慢地变化。由于输入的样本归一化了，所以计算loss时的images也需要归一化。代码如下：</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">41 cost = tf.reduce_mean(tf.pow( tf.cast(images,tf.float32)/255.0  - y_predt, 2))
42 optimizer = tf.train.AdamOptimizer(0.000001 ).minimize(cost)</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">5．建立session，运行</span>
</p>
<p class="ziti3">启动session，运行15 0000次。代码如下：</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">43 training_epochs =150000
44 display_step =200
45 
46 sess = tf.InteractiveSession()
47 sess.run(tf.global_variables_initializer())
48 
49 #启动队列
50 tf.train.start_queue_runners(sess=sess)
51 
52 # 启动循环开始训练
53 for epoch in range(training_epochs):
54     
55     _, c = sess.run([optimizer, cost])
56     # 显示训练中的详细信息
57     if epoch % display_step == 0:
58         d_batch=dbatch.eval()
59         mse,psnr=batch_mse_psnr(d_batch)
60         ypsnr=batch_y_psnr(d_batch)
61         ssim=batch_ssim(d_batch)
62         print("Epoch:", '%04d' % (epoch+1),
63               "cost=", "{:.9f}".format(c),"psnr",psnr,"ypsnr",ypsnr,"ssim",ssim)
64 
65 print("完成!")</pre>
<hr class="calibre6"/>
<p class="ziti3">在显示评估结果时，使用batch_mse_psnr、batch_y_psnr和batch_ssim这3个函数分别对节点dbatch的值进行运算，得到图片的质量评估值。</p>
<p class="ziti4">
<span class="yanse">6．构建图片质量评估函数</span>
</p>
<p class="ziti3">SR图片质量有其自己的一套评估质量算法：常用的两个指标是PSNR（Peak Signal- to-Noise Ratio）和SSIM（Structure Similarity Index）。这两个值越高，代表重建结果的像素值和标准越接近。对于PSNR的计算有两个方法：</p>
<p class="ziti4">·基于R、G、B，分别计算三通道中的MSE值再求平均值，然后再将结果代入求PSNR。</p>
<p class="ziti4">·基于YUV，求图像YUV空间中的Y分量，计算Y分量的PSNR值。</p>
<p class="ziti3">对于YUV的介绍如下：</p>
<p class="ziti3">YUV（亦称YCrCb）是另一种颜色编码方法，常被欧洲电视系统所采用。Y代表亮度信号，U（R-Y）与V（B-Y）分别代表两个色差信号。在没有U和V时，就会表现为只有亮度的黑白色，彩色电视采用YUV空间正是为了用亮度信号Y解决彩色电视机与黑白电视机的兼容问题，使黑白电视机也能接收彩色电视信号。</p>
<p class="ziti3">YUV和RGB互相转换的公式如下（RGB取值范围均为0～255）：</p>
<hr class="calibre6"/>
<pre class="ziti5">Y = 0.299R + 0.587G + 0.114B
U = -0.147R-0.289G + 0.436B
V = 0.615R-0.515G-0.100B
R = Y + 1.14V
G = Y-0.39U - 0.58V
B = Y + 2.03U</pre>
<hr class="calibre6"/>
<p class="ziti3">将这3个指标用代码实现如下。</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">66 def batch_mse_psnr(dbatch):
67     im1,im2=np.split(dbatch,2)
68     mse=((im1-im2)**2).mean(axis=(1,2))
69     psnr=np.mean(20*np.log10(255.0/np.sqrt(mse)))
70     return np.mean(mse),psnr
71 def batch_y_psnr(dbatch):
72     r,g,b=np.split(dbatch,3,axis=3)
73     y=np.squeeze(0.3*r+0.59*g+0.11*b)
74     im1,im2=np.split(y,2)
75     mse=((im1-im2)**2).mean(axis=(1,2))
76     psnr=np.mean(20*np.log10(255.0/np.sqrt(mse)))
77     return psnr
78 def batch_ssim(dbatch):
79     im1,im2=np.split(dbatch,2)
80     imgsize=im1.shape[1]*im1.shape[2]
81     avg1=im1.mean((1,2),keepdims=1)
82     avg2=im2.mean((1,2),keepdims=1)
83     std1=im1.std((1,2),ddof=1)
84     std2=im2.std((1,2),ddof=1)
85     cov=((im1-avg1)*(im2-avg2)).mean((1,2))*imgsize/(imgsize-1)
86     avg1=np.squeeze(avg1)
87     avg2=np.squeeze(avg2)
88     k1=0.01
89     k2=0.03
90     c1=(k1*255)**2
91     c2=(k2*255)**2
92     c3=c2/2
93     return np.mean((2*avg1*avg2+c1)*2*(cov+c3)/(avg1**2+avg2**2+c1)/
    (std1**2+std2**2+c2))</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">7．图示结果</span>
</p>
<p class="ziti3">与前面例子类似，将原始图片与函数变化的图片及模型输出的图片一并显示。这里先定义一个函数用来统一显示。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 必须要将其转化为UINT8的形式，否则图片会显示不出来。</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">94 def showresult(subplot,title,orgimg,thisimg,dopsnr = True):
95     p =plt.subplot(subplot)
96     p.axis('off') 
97     p.imshow(np.asarray(thisimg[0], dtype='uint8'))
98     if dopsnr :
99         conimg =  np.concatenate((orgimg,thisimg))
100         mse,psnr=batch_mse_psnr(conimg)
101         ypsnr=batch_y_psnr(conimg)
102         ssim=batch_ssim(conimg)
103         p.set_title(title+str(int(psnr))+" y:"+str(int(ypsnr))+" 
        s:"+str(ssim))
104     else:
105         p.set_title(title)</pre>
<hr class="calibre6"/>
<p class="ziti3">接着取一批次的图片放入模型，调用Showresult函数将生成的结果及评分值全部显示出来。</p>
<p class="ziti3">代码12-7　tfrecoderSRESPCN（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">106 imagesv, label_batch,x_smallv,x_nearestv,x_bilinv,x_bicubicv,y_predv 
= sess.run([images, labels,x_smalls,x_nearests,x_bilins,x_bicubics,
y_pred])
107 print("原",np.shape(imagesv),"缩放后的",np.shape(x_smallv),label_batch)
108 
109 #显示
110 plt.figure(figsize=(20,10))  
111 
112 showresult(161,"org",imagesv,imagesv,False)
113 showresult(162,"small/4",imagesv,x_smallv,False)
114 showresult(163,"near",imagesv,x_nearestv)
115 showresult(164,"biline",imagesv,x_bilinv)
116 showresult(165,"bicubicv",imagesv,x_bicubicv)
117 showresult(166,"pred",imagesv,y_predv)
118     
119 plt.show()</pre>
<hr class="calibre6"/>
<p class="ziti3">运行代码，输出如下，输出图片如图12-14所示。</p>
<hr class="calibre6"/>
<pre class="ziti5">……
Epoch: 144801 cost= 0.003637410 psnr 23.8877 ypsnr 24.0189 ssim 0.96434
Epoch: 145001 cost= 0.008538806 psnr 25.0453 ypsnr 25.3529 ssim 0.91564
Epoch: 145201 cost= 0.005899625 psnr 28.1946 ypsnr 29.1575 ssim 0.975755
Epoch: 145401 cost= 0.002309756 psnr 25.6251 ypsnr 25.5808 ssim 0.95208
Epoch: 145601 cost= 0.004211991 psnr 25.2114 ypsnr 25.2179 ssim 0.947036
Epoch: 145801 cost= 0.002519545 psnr 27.9464 ypsnr 28.9226 ssim 0.973354
Epoch: 146001 cost= 0.005268521 psnr 20.8838 ypsnr 20.7228 ssim 0.9175
Epoch: 146201 cost= 0.002536027 psnr 23.988 ypsnr 24.3302 ssim 0.934929
Epoch: 146401 cost= 0.003322446 psnr 28.2296 ypsnr 29.4929 ssim 0.908311
Epoch: 146601 cost= 0.007955125 psnr 25.5261 ypsnr 26.271 ssim 0.948738
Epoch: 146801 cost= 0.002779651 psnr 29.1436 ypsnr 30.8613 ssim 0.983795
Epoch: 147001 cost= 0.005602385 psnr 24.6309 ypsnr 25.1222 ssim 0.920429
Epoch: 147201 cost= 0.004883423 psnr 25.3241 ypsnr 25.7193 ssim 0.964581
Epoch: 147401 cost= 0.005192784 psnr 26.5626 ypsnr 26.9226 ssim 0.952263
Epoch: 147601 cost= 0.006907145 psnr 27.7884 ypsnr 28.4125 ssim 0.96983
Epoch: 147801 cost= 0.008132000 psnr 26.7713 ypsnr 27.9356 ssim 0.976656
Epoch: 148001 cost= 0.008132160 psnr 24.6795 ypsnr 26.011 ssim 0.96252
Epoch: 148201 cost= 0.003620633 psnr 24.5258 ypsnr 24.9886 ssim 0.943705
Epoch: 148401 cost= 0.008644918 psnr 21.6561 ypsnr 21.704 ssim 0.90348
Epoch: 148601 cost= 0.003554154 psnr 25.849 ypsnr 25.9136 ssim 0.97194
Epoch: 148801 cost= 0.003299494 psnr 23.6707 ypsnr 24.2183 ssim 0.959333
Epoch: 149001 cost= 0.003197462 psnr 23.7814 ypsnr 24.1327 ssim 0.913214
Epoch: 149201 cost= 0.001375712 psnr 26.5407 ypsnr 26.7266 ssim 0.957788
Epoch: 149401 cost= 0.003641539 psnr 25.5488 ypsnr 26.2268 ssim 0.925159
Epoch: 149601 cost= 0.003025041 psnr 25.2158 ypsnr 25.65 ssim 0.969086
Epoch: 149801 cost= 0.001514586 psnr 27.9188 ypsnr 28.6265 ssim 0.976076
完成!
原 (4, 200, 200, 3) 缩放后的 (4, 100, 100, 3) [3 0 0 1]</pre>
<hr class="calibre6"/>
<div class="pic"><img alt="" src="Image00278.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-14　ESPCN实例flowers结果</p>
<p class="ziti3">结果可见，使用ESPCN可以实现很好的SR效果。</p>
<p class="ziti3">本例仅将图片分辨率放大了2倍，而且与BILINEAR的比较优势不大，但没关系，下面就来演示一个更明显的例子，通过进一步优化网络将图片分辨率放大4倍。</p>
<h4 class="p3">12.8.4　实例95：使用残差网络的ESPCN</h4>
<p class="ziti3">在实例94中ESPCN与BILINEAR的结果比较优势没有那么明显，这是因为普通算法在仅仅放大两倍的图片处理上是很优秀的，另一个原因也是由于例子中的网络结构过于简单（仅三层）。下面通过对实例94的网络结构优化，实现在分辨率放大4倍任务上的图片重建。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">将flower数据集中的图片转成低分辨率，再通过使用带残差网络的ESPCN网络复原成高分辨率图片，并与其他复原函数的生成结果进行比较。</p>
<p class="ziti3">具体实现步骤如下。</p>
<p class="ziti4">
<span class="yanse">1．修改输入图片分辨率</span>
</p>
<p class="ziti3">在代码“12-7 tfrecoderSRESPCN.py”的基础上进行修改，将原来的输入尺寸由长宽各缩小一半变为长宽各缩小为原来的1/4。</p>
<p class="ziti3">代码12-8　resESPCN</p>
<hr class="calibre6"/>
<pre class="ziti5">01 ……
02 images, labels = tf.train.batch([distorted_image, label], batch_
size=batch_size)
03 print(images.shape)
04 
05 x_smalls = tf.image.resize_images(images, (np.int32(height/4), np.
int32(width/4)))#  尺寸变为原来的1/16
06 x_smalls2 = x_smalls/255.0
07 ……</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．添加残差网络</span>
</p>
<p class="ziti3">添加两个函数，一个是leaky_relu为leaky relu激活函数，另一个是用于生成网络残差块的函数residual_block，实现一个中间有两层卷积的残差块。接着在整个网络构造中，通过一个卷积层与一个残差层完成图像特征的转换。残差层是由16个残差块与一个卷积层组成的网络。特征转换之后再通过5层神经网络完成最终的特征修复处理过程，如图12-15所示。</p>
<div class="pic"><img alt="" src="Image00279.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-15　ResESPCN例子结构</p>
<p class="ziti3">图12-15中，最下面的5层为修复特征数据，第1层是一个卷积层，第2层会按照2×2大小的像素块将第一层的结果展开，第3层与第1层一样，第4层与第2层一样，第5层也是个卷积层。连续2次变换进行放大4倍的处理，最终通过输出3通道的卷积生成最终修复图片。</p>
<p class="ziti3">代码12-8　resESPCN （续）</p>
<hr class="calibre6"/>
<pre class="ziti5">08 def leaky_relu(x,alpha=0.1,name='lrelu'):
09      with tf.name_scope(name):
10          x=tf.maximum(x,alpha*x)
11          return x
12 def residual_block(nn,i,name='resblock'):
13     with tf.variable_scope(name+str(i)):
14         conv1=slim.conv2d(nn, 64, 3,activation_fn = leaky_relu,
        normalizer_fn=slim.batch_norm)
15         conv2=slim.conv2d(conv1, 64, 3,activation_fn = leaky_relu,
        normalizer_fn=slim.batch_norm)
16         return tf.add(nn,conv2)
17 
18 net = slim.conv2d(x_smalls2, 64, 5,activation_fn = leaky_relu)
19 block=[]
20 for i in range(16):
21     block.append(residual_block(block[-1] if i else net,i))
22 conv2=slim.conv2d(block[-1], 64, 3,activation_fn = leaky_relu,
normalizer_fn=slim.batch_norm)
23 sum1=tf.add(conv2,net)
24 
25 conv3=slim.conv2d(sum1, 256, 3,activation_fn = None)
26 ps1=tf.depth_to_space(conv3,2) 
27 relu2=leaky_relu(ps1)
28 conv4=slim.conv2d(relu2, 256, 3,activation_fn = None)
29 ps2=tf.depth_to_space(conv4,2) #再放大两倍 
30 relu3=leaky_relu(ps2)
31 y_predt=slim.conv2d(relu3, 3, 3,activation_fn = None) #输出</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">3．修改学习率，进行网络训练</span>
</p>
<p class="ziti3">将学习率改为0.001，同样使用AdamOptimizer优化方法，循环迭代100 000次开始训练。</p>
<p class="ziti3">代码12-8　resESPCN （续）</p>
<hr class="calibre6"/>
<pre class="ziti5">32 ……
33 learn_rate =0.001
34 
35 cost = tf.reduce_mean(tf.pow( tf.cast(images,tf.float32)/255.0  - 
y_predt, 2))
36 optimizer = tf.train.AdamOptimizer(learn_rate ).minimize(cost)
37 training_epochs =10000</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">4．添加检测点</span>
</p>
<p class="ziti3">网络结构的修改会使单次训练的时间变长，因此有必要添加检查点文件保存功能。先对变量flags赋值定义检查点保存的路径，在session中读取到检查点文件后解析出运行的迭代次数。在range中设置起始次数，让其继续训练。</p>
<p class="ziti3">代码12-8　resESPCN （续）</p>
<hr class="calibre6"/>
<pre class="ziti5">38 ……
39 flags='b'+str(batch_size)+'_h'+str(height/4)+'_r'+str(learn_
rate)+'_res' 
40 if not os.path.exists('save'):
41     os.mkdir('save')
42 save_path='save/tf_'+flags
43 if not os.path.exists(save_path):
44     os.mkdir(save_path)
45 saver = tf.train.Saver(max_to_keep=1) # 生成saver
46 
47 sess = tf.InteractiveSession()
48 sess.run(tf.global_variables_initializer())
49 
50 kpt = tf.train.latest_checkpoint(save_path)
51 print(kpt)
52 startepo= 0
53 if kpt!=None:
54     saver.restore(sess, kpt) 
55     ind = kpt.find("-")
56     startepo = int(kpt[ind+1:])
57     print("startepo=",startepo)
58 
59 #启动队列
60 tf.train.start_queue_runners(sess=sess)
61 
62 # 启动循环开始训练
63 for epoch in range(startepo,training_epochs):
64 ……
65         print("Epoch:", '%04d' % (epoch+1),
66               "cost=", "{:.9f}".format(c),"psnr",psnr,"ypsnr",ypsnr,"ssim",ssim)
67 
68         saver.save(sess, save_path+"/tfrecord.cpkt", global_
        step=epoch)
69 print("完成!")
70 saver.save(sess, save_path+"/tfrecord.cpkt", global_step=epoch)</pre>
<hr class="calibre6"/>
<p class="ziti3">在迭代指定次数后保存检查点，并且在全部训练结束后保存检查点文件。</p>
<p class="ziti3">运行整个代码生成结果如下，输出图片如图12-16所示。</p>
<hr class="calibre6"/>
<pre class="ziti5">……
Epoch: 4801 cost= 0.003252075 psnr 24.5383 ypsnr 25.332 ssim 0.956795592532
Epoch: 5201 cost= 0.002841802 psnr 26.1108 ypsnr 26.599 ssim 0.959523112064
Epoch: 5601 cost= 0.004468028 psnr 25.7363 ypsnr 26.3008 ssim 0.96185231328
Epoch: 6001 cost= 0.004859785 psnr 26.1274 ypsnr 26.7174 ssim 0.962346682679
Epoch: 6401 cost= 0.004147850 psnr 25.9059 ypsnr 26.7208 ssim 0.969348364467
Epoch: 6801 cost= 0.003628785 psnr 25.7018 ypsnr 26.4992 ssim 0.966612183827
Epoch: 7201 cost= 0.002464779 psnr 23.9676 ypsnr 24.4634 ssim 0.953078157091
Epoch: 7601 cost= 0.003710205 psnr 24.7987 ypsnr 25.4836 ssim 0.953239908046
Epoch: 8001 cost= 0.002421107 psnr 27.0966 ypsnr 28.1542 ssim 0.958096604393
Epoch: 8401 cost= 0.003401657 psnr 25.6712 ypsnr 26.4028 ssim 0.958035056742
Epoch: 8801 cost= 0.003299317 psnr 26.5742 ypsnr 27.1549 ssim 0.95990466244
Epoch: 9201 cost= 0.002930838 psnr 26.4124 ypsnr 26.9374 ssim 0.958304362037
Epoch: 9601 cost= 0.003253016 psnr 25.8007 ypsnr 26.2591 ssim 0.975577563284
完成!
原 (16,256,256,3)缩放后的(16,64,64,3)[0 2 4 1 0 4 1 3 0 0 1 1 3 1 0 2]</pre>
<hr class="calibre6"/>
<div class="pic"><img alt="" src="Image00280.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-16　resESPCN例子结果</p>
<p class="ziti3">图12-16中最后一幅图为模型生成的图片。放大4倍后可以看到，直接使用resize_ images生成的图片已经得不到很好的效果，但是通过模型生成的图片却有着同样高质量的清晰度。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 在构建相对较大型复杂网络结构（类似该例子的结构）进行训练时，检查点的设置是必须的，常常是运行一段，调节下参数，观察效果，然后再运行，再调节参数，再观察效果。这里有个技巧：如本例中将结构中的参数组成flags用于对检查点目录动态命名，这么做可以在调节参数时不用再额外考虑修改路径的问题，同时又能为不同参数对应的模型留下清晰的备份，便于比较。</p>
<p class="ziti4">
<span class="yanse">5．练习题</span>
</p>
<p class="ziti3">想一想，图12-16中从左向右的方向上，倒数第二幅图片为什么会如此显示？有什么办法能让其正常显示？</p>
<p class="ziti3">答案在12.8.3节中第3个小节的“注意”事项里，读者可以参考对应的代码完成该功能。</p>
<h4 class="p3">12.8.5　SRGAN的原理</h4>
<p class="ziti3">在图像放大4倍以上时，前面所介绍的方法得到的结果显得过于平滑，而缺少一些细节上的真实感。这是因为，传统方法使用的代价函数是基于像素点的最小均方差（MSE），该代价函数使重建结果有较高的信噪比，但是缺少了高频信息，所以会出现过度平滑的纹理。</p>
<p class="ziti3">SRGAN的思想是，使重建的高分辨率图像与真实的高分辨率图像，无论是低层次的像素值还是高层次的抽象特征及整体概念及风格上，都应当接近。</p>
<p class="ziti3">其中，对整体概念和风格的评估可以使用一个判别器，判断一幅高分辨率图像是由算法生成的还是真实的图像。如果一个判别器无法区分出来，那么由算法生成的图像就达到了对超分辨率修复成功的效果。</p>
<p class="ziti3">输入图片自身内容方面的损失值与来自对抗神经网络的损失值一起组成了最终的损失值（loss）。而对于自己的内容方面，基于像素点的平方差是一部分，另一部分是基于特征空间的平方差。基于特征空间特征的提取使用了VGG网络。</p>
<h4 class="p3">12.8.6　实例96：使用SRGAN实现flowers数据集的超分辨率修复</h4>
<p class="ziti3">本例中用SRGAN在有基于残差网络的ESPCN上面进行SR处理，观察它能带给我们怎样的效果。由于在计算生成器loss中的一部分需要使用VGG网络来提取特征，因此本例会用到第11章中的VGG19预训练模型。为了方便训练，这里直接使用了前面训练好的残差网络ESPCN网络模型作为生成器，用其生成的图片作为判别器的输入，通过GAN的机制进行二次优化。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">将flower数据集中的图片转为低分辨率，通过使用SRGAN网络将其还原成高分辨率，并与其他复原函数的生成结果进行比较。</p>
<p class="ziti3">具体实现步骤如下。</p>
<p class="ziti4">
<span class="yanse">1．引入头文件，图片预处理</span>
</p>
<p class="ziti3">在代码“12-8 resESPCN.py”基础上进行修改，引入slim中VGG网络头文件。样本部分与前面一样，只是增加了一个对输入的图片做归一化处理。</p>
<p class="ziti3">代码12-9　rsgan</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import tensorflow as tf
02 import time
03 import os 
04 import numpy as np
05 import matplotlib.pyplot as plt
06 from nets import vgg
07 ……
08 
09 images, labels = tf.train.batch([distorted_image, label], batch_
size=batch_size)
10 print(images.shape)
11 
12 images = tf.cast(images,tf.float32)
13 x_smalls=tf.image.resize_bicubic(images,[np.int32(height/4), np.
int32(width/4)]) #  变为原来的1/16
14 x_smalls2 = x_smalls/127.5-1 #将输入样本进行归一化处理</pre>
<hr class="calibre6"/>
<p class="ziti3">由于图片中每个像素都在0～255之间，所以除以255/2之后就会变为0～2之间的值，再减去1，就得到了x_smalls2。</p>
<p class="ziti4">
<span class="yanse">2．构建生成器</span>
</p>
<p class="ziti3">为了可以重用代码“12-8 resESPCN.py”中的模型，生成器的代码要与代码“12-8 resESPCN.py”保持一致，这里直接复制到gen函数中。</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">15 def gen(x_smalls2 ):
16     net = slim.conv2d(x_smalls2, 64, 5,activation_fn = leaky_relu)
17     block=[]
18     for i in range(16):
19         block.append(residual_block(block[-1] if i else net,i))
20     conv2=slim.conv2d(block[-1], 64, 3,activation_fn = leaky_relu,
    normalizer_fn=slim.batch_norm)
21     sum1=tf.add(conv2,net)
22     
23     conv3=slim.conv2d(sum1, 256, 3,activation_fn = None)
24     ps1=tf.depth_to_space(conv3,2) 
25     relu2=leaky_relu(ps1)
26     conv4=slim.conv2d(relu2, 256, 3,activation_fn = None)
27     ps2=tf.depth_to_space(conv4,2) #再放大两倍
28     relu3=leaky_relu(ps2)
29     y_predt=slim.conv2d(relu3, 3, 3,activation_fn = None) #输出
30     return y_predt</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">3．VGG的预输入处理</span>
</p>
<p class="ziti3">为了得到生成器基于内容的loss，要将生成的图片与真实图片分别输入VGG网络以获得它们的特征，然后在特征空间上计算loss。所以先将低分辨率图片作为输入放进生成器gen函数中，得到生成图片resnetimg，并将图片还原成0～255区间的正常像素值。同时准备好生成器的训练参数gen_var_list为后面优化器使用做准备。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 本例使用了一个新方法来提取已有模型的参数：①在生成器定义好后，获取一次图（运算任务）中的所有变量；②将已有模型载入判别器后，再获取一次图（运算任务）中的所有变量。由于两次执行的时间不一样，第一次得到的仅仅是生成器gen的变量，而第二次得到的是gen和判别器的变量总和，所以从变量总和中去掉第一次的变量剩下的就是判别器的变量。这么做的目的是为了再次使用前面例子里已经训练好的模型。</p>
<p class="ziti3">使用VGG模型时，必须在输入之前对图片做RGB均值的预处理。先定义处理RGB均值的函数，然后做具体变换。</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">31 def rgbmeanfun(rgb):
32     _R_MEAN = 123.68
33     _G_MEAN = 116.78
34     _B_MEAN = 103.94
35     print("build model started")
36     # 将 RGB 转化成BGR
37     red, green, blue = tf.split(axis=3, num_or_size_splits=3, 
    value=rgb)
38     rgbmean = tf.concat(axis=3, values=[red - _R_MEAN,green -_G_MEAN, 
    blue - _B_MEAN,])
39     return rgbmean
40     
41 resnetimg=gen(x_smalls2)
42 result=(resnetimg+1)*127.5
43 gen_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
44 
45 y_pred = tf.maximum(result,0)
46 y_pred = tf.minimum(y_pred,255)
47 
48 dbatch=tf.concat([images,result],0)
49 rgbmean = rgbmeanfun(dbatch)</pre>
<hr class="calibre6"/>
<p class="ziti3">RGB的处理与第11章中一样。具体的细节可以回看第11章的内容。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 这里将生成的图片与真实的图片通过维度为0的concat组合在一起处理。这是一个编写代码的小技巧。因为真实图片与生成的图片其处理过程是一样的（都要经过预处理，然后放到判别器中），所以就一起打包，这相当于两个batch的数据进行处理然后塞进判别式中，得到结果后再按照打包的先后顺序将它们分开即可。这种做法只用一套代码就可以完成真实图片和生成图片的处理。</p>
<p class="ziti4">
<span class="yanse">4．计算VGG特征空间的loss</span>
</p>
<p class="ziti3">VGG中的前5个卷积层用于特征提取，所以在使用时，只取其第5个卷积层的输出节点，其他的节点可以全部忽略。那么问题来了，如何能拿到模型中的指定节点呢？可以通过slim中nets文件夹下对应的VGG源码找到对应节点的名称。这里使用了一个更简单的方法：直接在models\slim\nets文件夹下打开“vgg_test.py”文件，在第50行中可以找到testEndPoints函数，其内容如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">……
def testEndPoints(self):
    batch_size = 5
    height, width = 224, 224
    num_classes = 1000
    with self.test_session():
      inputs = tf.random_uniform((batch_size,height, width, 3))
      _, end_points = vgg.vgg_19(inputs, num_classes)
      expected_names = [
          'vgg_19/conv1/conv1_1',
          'vgg_19/conv1/conv1_2',
          'vgg_19/pool1',
          'vgg_19/conv2/conv2_1',
          'vgg_19/conv2/conv2_2',
          'vgg_19/pool2',
          'vgg_19/conv3/conv3_1',
          'vgg_19/conv3/conv3_2',
          'vgg_19/conv3/conv3_3',
          'vgg_19/conv3/conv3_4',
          'vgg_19/pool3',
          'vgg_19/conv4/conv4_1',
          'vgg_19/conv4/conv4_2',
          'vgg_19/conv4/conv4_3',
          'vgg_19/conv4/conv4_4',
          'vgg_19/pool4',
          'vgg_19/conv5/conv5_1',
          'vgg_19/conv5/conv5_2',
          'vgg_19/conv5/conv5_3',
          'vgg_19/conv5/conv5_4',
          'vgg_19/pool5',
          'vgg_19/fc6',
          'vgg_19/fc7',
          'vgg_19/fc8'
      ]
      self.assertSetEqual(set(end_points.keys()), set(expected_names))</pre>
<hr class="calibre6"/>
<p class="ziti3">这是一个单元测试函数，里面列举了VGG19网络结构中所有的节点名称。如上代码'vgg_19/conv5/conv5_4' 就是本例中想要的节点，直接将该字符串复制放到本例代码中，如下所示。</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">50 #vgg 特征值
51 _, end_points = vgg.vgg_19(rgbmean, num_classes=1000,is_training=
False,spatial_squeeze=False)
52 conv54=end_points['vgg_19/conv5/conv5_4']
53 print("vgg.conv5_4",conv54.shape)
54 fmap=tf.split(conv54,2)
55 
56 content_loss=tf.losses.mean_squared_error(fmap[0],fmap[1])</pre>
<hr class="calibre6"/>
<p class="ziti3">由于前面通过concat将两个图片放一起来处理，得到结果后，还要使用split将其分开，接着通过平方差算出基于特征空间的loss。</p>
<p class="ziti4">
<span class="yanse">5．判别器的构建</span>
</p>
<p class="ziti3">判别器主要是通过一系列卷积层组合起来所构成的，最终使用两个全连接层实现映射到一维的输出结果。具体函数实现如下：</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">57 def Discriminator(dbatch, name ="Discriminator"):
58     with tf.variable_scope(name):
59         net = slim.conv2d(dbatch, 64, 1,activation_fn = leaky_relu)
60 
61         ochannels=[64,128,128,256,256,512,512]
62         stride=[2,1]
63 
64         for i in range(7):
65             net = slim.conv2d(net, ochannels[i], 3,stride = stride
            [i%2],activation_fn = leaky_relu,normalizer_fn=slim.
            batch_norm,scope='block'+str(i))
66 
67         dense1 = slim.fully_connected(net, 1024, activation_
        fn=leaky_relu)
68         dense2 = slim.fully_connected(dense1, 1, activation_
        fn=tf.nn.sigmoid)
69         
70         return dense2</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">6．计算loss，定义优化器</span>
</p>
<p class="ziti3">将判别器的结果裁开，分别得到真实图片与生成图片判别的结果，以LSGAN的方式计算生成器与判别器的loss，在生成器loss中加入基于特征空间的loss。按照前面第3步中所讲的训练参数的获取方式获得判别器训练参数disc_var_list，使用AdamOptimizer优化loss值。代码如下：</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">71 disc=Discriminator(dbatch)
72 D_x,D_G_z=tf.split(tf.squeeze(disc),2)   
 
73   
74 adv_loss=tf.reduce_mean(tf.square(D_G_z-1.0))
75 
76 gen_loss=(adv_loss+content_loss)
77 disc_loss=(tf.reduce_mean(tf.square(D_x-1.0)+tf.square(D_G_z)))
78 
79 disc_var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
80 print("len-----",len(disc_var_list),len(gen_var_list))
81 for x in gen_var_list:
82     disc_var_list.remove(x)
83 
84 learn_rate =0.001
85 global_step=tf.Variable(0,trainable=0,name='global_step')
86 gen_train_step=tf.train.AdamOptimizer(learn_rate).minimize
(gen_loss,global_step,gen_var_list)
87 disc_train_step=tf.train.AdamOptimizer(learn_rate).minimize
(disc_loss,global_step,disc_var_list)</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">7．指定准备载入的预训练模型路径</span>
</p>
<p class="ziti3">这次需要对3个检查点路径进行配置，第一个是本程序的SRGAN检查点文件，第二个是srResNet检查点文件，最后一个是VGG模型文件。</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">88 #残差网络检查点文件相关定义
89 flags='b'+str(batch_size)+'_r'+str(np.int32(height/4))+'_r'+str
(learn_rate)+'rsgan'
90 save_path='save/srgan_'+flags
91 if not os.path.exists(save_path):
92     os.mkdir(save_path)
93 saver = tf.train.Saver(max_to_keep=1) # 生成saver
94 
95 srResNet_path='./save/tf_b16_h64.0_r0.001_res/'
96 srResNetloader = tf.train.Saver(var_list=gen_var_list) # 生成saver
97 
98 #VGG检查点
99 checkpoints_dir = 'vgg_19_2016_08_28'
100 init_fn = slim.assign_from_checkpoint_fn(
101     os.path.join(checkpoints_dir, 'vgg_19.ckpt'),
102     slim.get_model_variables('vgg_19'))</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 这里对于srResNet的变量恢复，要在建立Saver时传入var_list参数来指定好所要恢复的变量列表，否则默认是恢复所有变量，但是模型里却找不到其他变量，会报错误。</p>
<p class="ziti4">
<span class="yanse">8．启动session从检查点恢复变量</span>
</p>
<p class="ziti3">对于VGG模型的恢复，可以参考第11章中的内容。其他的检查点恢复的写法与前面一样。代码如下：</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">103 log_steps=100
104 training_epochs=16000
105 
106 with tf.Session() as sess:
107  
108     sess.run(tf.global_variables_initializer())  
109     
110     init_fn(sess)    
111     
112     kpt = tf.train.latest_checkpoint(srResNet_path)
113     print("srResNet_path",kpt,srResNet_path)
114     startepo= 0
115     if kpt!=None:
116         srResNetloader.restore(sess, kpt) 
117         ind = kpt.find("-")
118         startepo = int(kpt[ind+1:])
119         print("srResNetloader global_step=",global_step.eval(),startepo)     
120     
121     kpt = tf.train.latest_checkpoint(save_path)
122     print("srgan",kpt)
123     startepo= 0
124     if kpt!=None:
125         saver.restore(sess, kpt) 
126         ind = kpt.find("-")
127         startepo = int(kpt[ind+1:])
128         print("global_step=",global_step.eval(),startepo) </pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">9．启动带协调器的队列线程，开始训练</span>
</p>
<p class="ziti3">本例中涉及的参数比较多，模型比较大，会导致每次迭代时间都很长，所以加入检测点是非常有必要的。这里涉及检查点保存的粒度，如间隔太短，因为频繁地写文件会减慢训练速度，如果设置的间隔太长，中途如发生意外暂停会导致浪费了一部分训练时间，可以通过try的方式在异常捕获时再保存一次检查点，这样可以把中途的训练结果保存下来。</p>
<p class="ziti3">代码12-9　rsgan（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">129 coord = tf.train.Coordinator()
130     threads = tf.train.start_queue_runners(sess, coord)
131     
132     try:
133         def train(endpoint,gen_step,disc_step):
134             while global_step.eval()&lt;=endpoint:
135                 
136                 if((global_step.eval()/2)%log_steps==0): # 一次走两步
137                    
138                     d_batch=dbatch.eval()
139                     mse,psnr=batch_mse_psnr(d_batch)
140                     ssim=batch_ssim(d_batch)
141                     s=time.strftime('%Y-%m-%d %H:%M:%S:',time.localtime
                    (time.time()))+'step='+str(global_step.eval())+' 
                     mse='+str(mse)+' psnr='+str(psnr)+' ssim='+str
                     (ssim)+' gen_loss='+str(gen_loss.eval())+' disc_
                     loss='+str(disc_loss.eval())
142                     print(s)
143                     f=open('info.train_'+flags,'a')
144                     f.write(s+'\n')
145                     f.close()
146                     saver.save(sess, save_path+"/srgan.cpkt", global_
                    step=global_step.eval())
147                     
148                 sess.run(disc_step)
149                 sess.run(gen_step)
150         train(training_epochs,gen_train_step,disc_train_step)
151         print('训练完成')
152 ……          #显示部分同resEspcn例子，代码省略
153     except tf.errors.OutOfRangeError:
154         print('Done training -- epoch limit reached')
155     except KeyboardInterrupt:
156         print("Ending Training...")
157         saver.save(sess, save_path+"/srgan.cpkt", global_
        step=global_step.eval())
158     finally:
159         coord.request_stop()
160 
161     coord.join(threads)</pre>
<hr class="calibre6"/>
<p class="ziti3">运行代码，生成结果如图12-17所示。</p>
<div class="pic"><img alt="" src="Image00281.jpg" class="calibre4"/>
</div>
<p class="middle-img">图12-17　SRGAN例子结果</p>
<p class="ziti3">图12-17中最后一张是模型生成的结果，每张图片上都有其评分值，可以看到SRGAN得到的PSNR和SSIM评价值不是最高的。但是我们肉眼看上去确实清晰了不少，并且通过有关机构对其进行MOS（Mean Opinion Score）的评价也表明，SRGAN生成的高分辨率图像看起来更真实。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 MOS（mean opinion score）采用主观评定和技术评定相结合的方式。所谓主观评定就是有人为的参与，用人来评定。</p>
<p class="ziti3">该例中只演示了运行迭代1万多次的效果。如果将ResEspcn的模型与srGAN的模型分别加一个数量级，还会得到效果更优质的图片，有兴趣的读者可以自行尝试。</p>
<div class="calibre1">本书由「<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body>
</html>
