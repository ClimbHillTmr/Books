<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>未知</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h3 class="p">8.3　网络结构</h3>
<p class="ziti3">卷积神经网络的结构与全连接网络相比复杂得多。它的网络结构主要包括卷积层、池化层。细节又可以分为滤波器、步长、卷积操作、池化操作等。</p>
<h4 class="p3">8.3.1　网络结构描述</h4>
<p class="ziti3">前面讲述的是一个基本原理，实际的卷积操作会复杂一些，对于一幅图片一般会使用多个卷积核（滤波器），将它们统一放到卷积层里来操作，这一层中有几个滤波器，就会得出几个feature map，接着还要经历一个池化层（pooling），将生成的feature map缩小（降维），池化层会在下面的文章中介绍。图8-3所示为神经网络中一个标准的卷积操作组合。</p>
<div class="pic"><img alt="" src="Image00129.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-3　卷积结构</p>
<p class="ziti3">图8-3中卷积层里面channel的个数代表卷积层的深度。池化层中则只有一个滤波器（fileter），主要参数是尺寸大小（即步长大小）。</p>
<p class="ziti3">下面先来看一个卷积网络的完整结构，如图8-4所示。</p>
<div class="pic"><img alt="" src="Image00130.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-4　卷积网络的完整结构</p>
<p class="ziti3">一个卷积神经网络里包括5部分——输入层、若干个卷积操作和池化层结合的部分、全局平均池化层、输出层：</p>
<p class="ziti4">·输入层：将每个像素代表一个特征节点输入进来。</p>
<p class="ziti4">·卷积操作部分：由多个滤波器组合的卷积层。</p>
<p class="ziti4">·池化层：将卷积结果降维。</p>
<p class="ziti4">·全局平均池化层：对生成的feature map取全局平均值。</p>
<p class="ziti4">·输出层：需要分成几类，相应的就会有几个输出节点。每个输出节点都代表当前样本属于的该类型的概率。</p>
<p class="ziti3">输入层、输出层在前面章节已有介绍，下面重点讲讲卷积操作和池化层。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 全局平均池化层是后出的新技术，在以前的大部分教材里，这个位置通过是使用1～3个全连接层来代替的。全连接层的劣势在于会产生大量的计算，需要大量的参数，但在效果上却和全局平均池化层一样。所以，在这里请读者忘掉全连接层，直接使用效率更高的全局平均池化层。</p>
<h4 class="p3">8.3.2　卷积操作</h4>
<p class="ziti3">前面的8.2节中采用soble因子对图片的操作，可以理解成一次卷积操作。下面来系统地了解卷积操作。卷积分为窄卷积、全卷积和同卷积。</p>
<p class="ziti3">在一一介绍这些卷积类型之前，首先介绍一下步长的概念。</p>
<p class="ziti4">
<span class="yanse">1．步长</span>
</p>
<p class="ziti3">步长是卷积操作的核心。通过步长的变换，可以得到想要的不同类型的卷积操作。先以窄卷积为例，看看它的操作及相关术语，如图8-5所示。</p>
<div class="pic"><img alt="" src="Image00131.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-5　卷积细节</p>
<p class="ziti3">图8-5中，5×5大小的矩阵代表图片，每个图片右侧的3×3矩阵代表卷积核，最右侧的3×3矩阵为计算完的结果feature map。</p>
<p class="ziti3">卷积操作仍然是将卷积核（filter）对应的图片（image）中的矩阵数据一一相乘，再相加。图8-5中，第一行feature map中的第一个元素，是由image块中前3行3列中的每个元素与filter中的对应元素相乘再相加得到的（4=1×1+1×0+1×1+0×0+1×1+1×0+0×1+0×0+1×1）。</p>
<p class="ziti3">步长（stride）表示卷积核在图片上移动的格数。</p>
<p class="ziti4">·当步长为1的情况下，如图8-5中，第二行右边的feature map块里的第二个元素3，是由卷积核计算完第一个元素4，右移一格后计算得来的，相当于图片中的前3行和第1到第4列围成的3×3矩阵与卷积核各对应元素进行相乘相加操作（3= 1×1+1×0+0×1+1×0+1×1+1×0+0×1+1×0+1×1）。</p>
<p class="ziti4">·当步长为2的情况下，就代表每次移动2个格，最终会得到一个如图8-5中第二行左边的2×2矩阵块的结果。</p>
<p class="ziti4">
<span class="yanse">2．窄卷积</span>
</p>
<p class="ziti3">窄卷积（valid卷积），从字面上也可以很容易理解，即生成的feature map比原来的原始图片小，它的步长是可变的。假如滑动步长为S，原始图片的维度为N1×N1，那么卷积核的大小为N2×N2，卷积后的图像大小（N1-N2）/S+1×（N1-N2）/S+1。</p>
<p class="ziti4">
<span class="yanse">3．同卷积</span>
</p>
<p class="ziti3">同卷积（same卷积），代表的意思是卷积后的图片尺寸与原始图片的尺寸一样大，同卷积的步长是固定的，滑动步长为1。一般操作时都要使用padding技术（外围补一圈0，以确保生成的尺寸不变）。</p>
<p class="ziti4">
<span class="yanse">4．全卷积</span>
</p>
<p class="ziti3">全卷积（full卷积），也叫反卷积，就是把原始图片里的每个像素点都用卷积操作展开。如图8-6所示，白色的块是原始图片，浅色的是卷积核，深色的是正在卷积操作的像素点。反卷积操作的过程中，同样需要对原有图片进行padding操作，生成的结果会比原有的图片尺寸大。</p>
<div class="pic"><img alt="" src="Image00132.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-6　反卷积</p>
<p class="ziti3">全卷积的步长也是固定的，滑动步长为1，假如原始图片的维度为N1×N1，那么卷积核的大小为N2×N2，卷积后的图像大小，即N1+N2-1×N1+N2-1</p>
<p class="ziti3">前面的窄卷积和同卷积都是卷积网络里常用的技术，然而全卷积（full卷积）却相反，它更多地用在反卷积网络中，关于反卷积网络的内容，将在后面的章节进行介绍。</p>
<p class="ziti4">
<span class="yanse">5．反向传播</span>
</p>
<p class="ziti3">因为反向传播在框架里已经封装好，不需要对其进行编码修改，所以对于反向传播方面的知识，这里只简单介绍下基本原理，读者知道大概意思即可。</p>
<p class="ziti3">反向传播的核心步骤主要有两步：</p>
<p class="ziti3">（1）反向将误差传到前面一层。</p>
<p class="ziti3">（2）根据当前的误差对应的学习参数表达式，计算出其需要更新的差值。</p>
<p class="ziti3">对于第（2）步，与前面的反向求导是一样的，仍然使用链式求导法则，找到使误差最小化的梯度，再配合学习率算出更新的差值。将生成的feature map做一次padding后，与转置后的卷积核做一次卷积操作即可得到输入端的误差，从而实现误差的反向传递。</p>
<p class="ziti3">这里只介绍个概念，具体的计算规则请读者参看后面的反卷积部分，这里不再赘述。</p>
<p class="ziti4">
<span class="yanse">6．多通道的卷积</span>
</p>
<p class="ziti3">通道（Channel），是指图片中每个像素由几个数来表示，这几个数一般指的就是色彩。比如一个灰度图的通道就是1，一个彩色图的通道就是3（红、黄、蓝）。前面介绍的都是单通道的卷积计算，那么对于多通道的卷积计算是什么样的呢？</p>
<p class="ziti3">在卷积神经网络里，通道又分输入通道和输出通道。</p>
<p class="ziti4">·输入通道：就是前面刚介绍的图片的通道。如是彩色图片，起始的输入通道就是3。如是中间层的卷积，输入通道就是上一层的输出通道个数，计算方法是，每个输入通道的图片都使用同一个卷积核进行卷积操作，生成与输入通道匹配的feature map（比如彩色图片就是3个），然后再把这几张feature map相同位置上的值加起来，生成一张feature map。</p>
<p class="ziti4">·输出通道：很好理解了，想要输出几个feature map，就放几个卷积核，就是几个输出通道。</p>
<h4 class="p3">8.3.3　池化层</h4>
<p class="ziti3">池化的主要目的是降维，即在保持原有特征的基础上最大限度地将数组的维数变小。</p>
<p class="ziti3">池化的操作外表跟卷积很像，只是算法不同：</p>
<p class="ziti4">·卷积是将对应像素上的点相乘，然后再相加。</p>
<p class="ziti4">·池化中只关心滤波器的尺寸，不考虑内部的值。算法是，滤波器映射区域内的像素点取取平均值或最大值。</p>
<p class="ziti3">池化步骤也有步长，这一点与卷积是一样的。</p>
<p class="ziti4">
<span class="yanse">1．均值池化</span>
</p>
<p class="ziti3">这个很好理解，就是在图片上对应出滤波器大小的区域，对里面的所有不为0的像素点取均值。这种方法得到的特征数据会对背景信息更敏感一些。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 一定是不为0的像素点，这个很重要。如果把带0的像素点加上，则会增加分母，从而使整体数据变低。</p>
<p class="ziti4">
<span class="yanse">2．最大池化</span>
</p>
<p class="ziti3">同理，最大池化就是在图片上对应出滤波器大小的区域，将里面的所有像素点取最大值。这种方法得到的特征数据会对纹理特征的信息更敏感一些。</p>
<p class="ziti4">
<span class="yanse">3．反向传播</span>
</p>
<p class="ziti3">池化的反向传播要比卷积容易理解。对于最大池化，直接将其误差还原到对应的位置，其他用0填入；对于均值池化，则是将其误差全部填入该像素对应的池化区域。该部分的详细算法也与反池化算法完全相同，读者可以参看反池化部分的介绍。</p>
<div class="calibre1">本书由「<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body>
</html>
