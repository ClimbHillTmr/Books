<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>未知</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<h3 class="p">8.4　卷积神经网络的相关函数</h3>
<p class="ziti3">在TensorFlow中，使用tf.nn.conv2d来实现卷积操作，使用tf.nn.max_pool进行最大池化操作。通过传入不同的参数，来实现各种不同类型的卷积与池化操作。下面介绍这两个函数中各参数的具体意义。</p>
<h4 class="p3">8.4.1　卷积函数tf.nn.conv2d</h4>
<p class="ziti3">TensorFlow里使用tf.nn.conv2d函数来实现卷积，其格式如下。</p>
<hr class="calibre6"/>
<pre class="ziti5">tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)</pre>
<hr class="calibre6"/>
<p class="ziti3">除去参数name参数用以指定该操作的name，与方法有关的共有5个参数。</p>
<p class="ziti4">·input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch，in_height，in_width，in_channels]这样的形状（shape），具体含义是“训练时一个batch的图片数量，图片高度，图片宽度，图像通道数”，注意这是一个四维的Tensor，要求类型为float32和float64其中之一。</p>
<p class="ziti4">·filter：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height，filter_width，in_channels，out_channels]这样的shape，具体含义是“卷积核的高度，滤波器的宽度，图像通道数，滤波器个数”，要求类型与参数input相同。有一个地方需要注意，第三维in_channels，就是参数input的第四维。</p>
<p class="ziti4">·strides：卷积时在图像每一维的步长，这是一个一维的向量，长度为4。</p>
<p class="ziti4">·padding：定义元素边框与元素内容之间的空间。string类型的量，只能是SAME和VALID其中之一，这个值决定了不同的卷积方式，padding的值为'VALID'时，表示边缘不填充，当其为'SAME'时，表示填充到滤波器可以到达图像边缘。</p>
<p class="ziti4">·use_cudnn_on_gpu：bool类型，是否使用cudnn加速，默认为true。</p>
<p class="ziti4">·返回值：tf.nn.conr2d函数结果返回一个Tensor，这个输出就是常说的feature map。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 在卷积函数中，padding参数是最容易引起歧义的，该参数仅仅决定是否要补0，因此一定要清楚padding设为SAME的真正含义。在设为SAME的情况下，只有在步长为1时生成的feature map才会与输入值相等。</p>
<h4 class="p3">8.4.2　padding规则介绍</h4>
<p class="ziti3">padding属性的意义是定义元素边框与元素内容之间的空间。</p>
<p class="ziti3">在tf.nn.conv2d函数中，当变量padding为VALID和SAME时，函数具体是怎么计算的呢？其实是有公式的。为了方便演示，先来定义几个变量：</p>
<p class="ziti4">·输入的尺寸中高和宽定义成in_height、in_width。</p>
<p class="ziti4">·卷积核的高和宽定义成filter_height、filter_width。</p>
<p class="ziti4">·输出的尺寸中高和宽定义成output_height、output_width。</p>
<p class="ziti4">·步长的高宽方向定义成strides_height、strides_ width。</p>
<p class="ziti4">
<span class="yanse">1．VALID情况</span>
</p>
<p class="ziti3">输出宽和高的公式代码分别为：</p>
<hr class="calibre6"/>
<pre class="ziti5">output_width=(in_width–filter_width + 1)/strides_ width（结果向上取整）
output_height=(in_height–filter_height+1)/strides_height（结果向上取整）</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．SAME情况</span>
</p>
<p class="ziti3">输出的宽和高将与卷积核没有关系，具体公式代码如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">out_height = in_height / strides_height（结果向上取整）
out_width = in_width / strides_ width（结果向上取整）</pre>
<hr class="calibre6"/>
<p class="ziti3">这里有一个很重要的知识点——补零的规则，见如下代码：</p>
<hr class="calibre6"/>
<pre class="ziti5">pad_height=max((out_height-1)×strides_height +filter_height-in_height,0)
pad_width = max((out_width-1)×strides_ width +filter_width - in_width, 0)
pad_top = pad_height / 2
pad_bottom = pad _height - pad_top
pad_left = pad _width / 2
pad_right = pad _width - pad_left</pre>
<hr class="calibre6"/>
<p class="ziti3">上面代码中</p>
<p class="ziti4">·pad_height：代表高度方向要填充0的行数。</p>
<p class="ziti4">·pad_width：代表宽度方向要填充0的列数。</p>
<p class="ziti4">·pad_top、pad_bottom、pad_left、pad_right：分别代表上、下、左、右这4个方向填充0的行、列数。</p>
<p class="ziti4">
<span class="yanse">3．规则举例</span>
</p>
<p class="ziti3">下面通过例子来理解一下padding规则。</p>
<p class="ziti3">假设用`一个一维数据来举例，输入是13，filter是6，步长是5，对于padding的取值有如下表示：</p>
<hr class="calibre6"/>
<pre class="ziti5">'VALID'相当于padding，生成的宽度为（13-6+1）/5 = 2（向上取整）个数字。
   inputs:         1  2  3  4  5  6  7  8  9  10  11  (12 13)
                 |________________|                  dropped
                               |_________________|</pre>
<hr class="calibre6"/>
<p class="ziti3">'SAME'=相当于padding，生成的宽度为13/5=3（向上取整）个数字。</p>
<p class="ziti3">Padding的方式可以如下计算：</p>
<hr class="calibre6"/>
<pre class="ziti5">Pad_width = (3-1) ×5+6-13 = 3
Pad_left = pad_width/2= 3/2 = 1
Pad_rigth = pad_width-pad_left = 2</pre>
<hr class="calibre6"/>
<p class="ziti3">在左边补一个0，右边补2个0。</p>
<hr class="calibre6"/>
<pre class="ziti5">                 pad|                                          | pad
   inputs:      0 | 1  2  3  4  5  6  7  8  9  10  11  12  13 | 0  0  
              |________________|
                            |_________________|
                                           |_____________________|</pre>
<hr class="calibre6"/>
<h4 class="p3">8.4.3　实例37：卷积函数的使用</h4>
<p class="ziti3">下面通过一个例子来介绍卷积函数的用法。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">通过手动生成一个5×5的矩阵来模拟图片，定义一个2×2的卷积核，来测试tf.nn.conv2d函数里的不同参数，验证其输出结果。</p>
<p class="ziti3">在这个例子中，分为如下几个步骤来写代码。</p>
<p class="ziti3">（1）定义输入变量。</p>
<p class="ziti3">（2）定义卷积核变量。</p>
<p class="ziti3">（3）定义卷积操作。</p>
<p class="ziti3">（4）运行卷积操作。</p>
<p class="ziti3">下面就来一一操作。</p>
<p class="ziti4">
<span class="yanse">1．定义输入变量</span>
</p>
<p class="ziti3">定义3个输入变量用来模拟输入图片，分别是5×5大小1个通道的矩阵、5×5大小2个通道的矩阵、4×4大小1个通道的矩阵，并将里面的值统统赋为1。</p>
<p class="ziti3">代码8-1　卷积函数使用</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import tensorflow as tf  
02   
03 # [batch, in_height, in_width, in_channels] [训练时一个批次的图片数量, 
图片高度, 图片宽度, 图像通道数]  
04 input = tf.Variable(tf.constant(1.0,shape = [1, 5, 5, 1])) 
05 input2 = tf.Variable(tf.constant(1.0,shape = [1, 5, 5, 2]))
06 input3 = tf.Variable(tf.constant(1.0,shape = [1, 4, 4, 1]))</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．定义卷积核变量</span>
</p>
<p class="ziti3">定义5个卷积核，每个卷积核都是2×2的矩阵，只是输入、输出的通道数有差别，分别为1ch输入、1ch输出，1ch输入、2ch输出，1ch输入、3ch输出，2ch输入、2ch输出，2ch输入、1ch输出，并分别在里面填入指定的数值：</p>
<p class="ziti3">代码8-1　卷积函数使用（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">07 # [filter_height, filter_width, in_channels, out_channels]
（卷积核的高度，卷积核的宽度，图像通道数，卷积核个数）
08 filter1 =  tf.Variable(tf.constant([-1.0,0,0,-1],shape = [2, 2, 1, 1]))
09 filter2 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1],shape = 
[2, 2, 1, 2])) 
10 filter3 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1,-1.0,
0,0,-1],shape = [2, 2, 1, 3])) 
11 filter4 =  tf.Variable(tf.constant([-1.0,0,0,-1,
12                                    -1.0,0,0,-1,
13                                    -1.0,0,0,-1,
14                                    -1.0,0,0,-1],shape = [2, 2, 2, 2])) 
15 filter5 =  tf.Variable(tf.constant([-1.0,0,0,-1,-1.0,0,0,-1],shape = 
[2, 2, 2, 1]))</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">3．定义卷积操作</span>
</p>
<p class="ziti3">将步骤1的输入与步骤2的卷积核组合起来，建立8个卷积操作，看看生成的内容与前面所讲述的规则是否一致。</p>
<p class="ziti3">代码8-1　卷积函数使用（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">16 # padding的值为'VALID'，表示边缘不填充； 当其为'SAME'时，表示填充到卷积核可以到
达图像边缘  
17 op1 = tf.nn.conv2d(input, filter1, strides=[1, 2, 2, 1], padding='SAME') #1个通道输入，生成1个feature ma
18 op2 = tf.nn.conv2d(input, filter2, strides=[1, 2, 2, 1], padding='SAME') 
#1个通道输入，生成2个feature map
19 op3 = tf.nn.conv2d(input, filter3, strides=[1, 2, 2, 1], padding='SAME') #1个通道输入，生成3个feature map
20 
21 op4 = tf.nn.conv2d(input2, filter4, strides=[1, 2, 2, 1], padding=
'SAME') # 2个通道输入，生成2个feature
22 op5 = tf.nn.conv2d(input2, filter5, strides=[1, 2, 2, 1], padding=
'SAME') # 2个通道输入，生成1个feature map
23 
24 vop1 = tf.nn.conv2d(input, filter1, strides=[1, 2, 2, 1], padding=
'VALID') # 5*5 对于pading不同而不同
25 op6 = tf.nn.conv2d(input3, filter1, strides=[1, 2, 2, 1], padding=
'SAME') 
26 vop6 = tf.nn.conv2d(input3, filter1, strides=[1, 2, 2, 1], padding=
'VALID')   #4*4与pading无关</pre>
<hr class="calibre6"/>
<p class="ziti3">这么多卷积操作看着有点混乱，按照演示的目的将其分类一下，分别介绍。</p>
<p class="ziti3">（1）演示padding补0的情况</p>
<p class="ziti3">如上文代码，op1使用了padding=SAME的一个通道输入、一个通道输出的卷积操作，步长为2×2，按前面的函数介绍，这种情况TensorFlow会对input补0。通过前面的公式计算，会生成3×3大小的矩阵，并且在右侧和下侧各补一圈0，由5×5矩阵变成6×6矩阵，如图8-7所示。</p>
<div class="pic"><img alt="" src="Image00133.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-7　padding例子</p>
<p class="ziti3">（2）演示多通道输出时的内存排列</p>
<p class="ziti3">op2示例了1个通道生成2个输出，oP3示例了1个通道生成3个输出，可以看下它们在内存中的排列样子。</p>
<p class="ziti3">（3）演示卷积核对多通道输入的卷积处理</p>
<p class="ziti3">op4示例了2个通道生成2个输出，op5示例了2个通道生成1个输出，比较下对于2个通道的卷积结果，观察是多通道的结果叠加，还是每个通道单独对应一个卷积核进行输出。</p>
<p class="ziti3">（4）验证不同尺寸下的输入受到padding为SAME和VALID的影响</p>
<p class="ziti3">op1和vop1示例了5×5尺寸输入在padding为SAME和VALID时的变化情况，op6 和vop6示例了4×4尺寸输入在padding为SAME和VALID下的变化情况。</p>
<p class="ziti4">
<span class="yanse">4．运行卷积操作</span>
</p>
<p class="ziti3">在本步操作之前，读者可以把前面的规则熟记一下，然后试着自己推导一下，比较得到的输出结果。下面把这些结果打印出来，看看与你推导的是否一致。</p>
<p class="ziti3">代码8-1　卷积函数使用（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">27 init = tf.global_variables_initializer()  
28 with tf.Session() as sess:  
29     sess.run(init)  
30     
31     print("op1:\n",sess.run([op1,filter1]))    #1-1  后面补0
32     print("------------------")
33     
34     print("op2:\n",sess.run([op2,filter2]))   #1-2多卷积核，按列取
35     print("op3:\n",sess.run([op3,filter3]))   #1-3一个输入，3个输出
36     print("------------------")   
37     
38     print("op4:\n",sess.run([op4,filter4]))   #2-2通道叠加
39     print("op5:\n",sess.run([op5,filter5]))   #2-1两个输入，一个输出
40     print("------------------")
41   
42     print("op1:\n",sess.run([op1,filter1]))    #1-1一个输入，一个输出
43     print("vop1:\n",sess.run([vop1,filter1]))
44     print("op6:\n",sess.run([op6,filter1]))
45     print("vop6:\n",sess.run([vop6,filter1]))</pre>
<hr class="calibre6"/>
<p class="ziti3">下面分别是介绍这段代码的执行结果。</p>
<p class="ziti3">（1）执行代码8-1中的31和32行代码，对应的输出如下：（为了看起来方便，将格式进行了整理）</p>
<hr class="calibre6"/>
<pre class="ziti5">op1:
 [array([[[[-2.],[-2.], [-1.]],
        [[-2.],[-2.],[-1.]],
        [[-1.],[-1.],[-1.]]]], dtype=float32),
 array([[[[-1.]],[[ 0.]]],
   [[[ 0.]],[[-1.]]]], dtype=float32)]</pre>
<hr class="calibre6"/>
<p class="ziti3">上面输出中5×5矩阵通过卷积操作生成了3×3矩阵，对padding的补0情况是在后面和下面补0，所以会在矩阵的右边和下边生成-1。</p>
<p class="ziti3">（2）执行代码8-1中的34～36行，对应的输出如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">op2:
 [array([[[[-2.,-2.],[-2.,-2.],[-2.,0.]],
        [[-2.,-2.],[-2.,-2.],[-2.,0.]],
        [[-1.,-1.],[-1.,-1.],[-1.,0.]]]],dtype=float32), 
array([[[[-1.,0.]],[[0.,-1.]]],
[[[-1.,0.]],[[0.,-1.]]]],dtype=float32)]
op3:
 [array([[[[-2.,-2.,-2.],[-2.,-2.,-2.],[-1.,-1.,-1.]],
        [[-2.,-2.,-2.],[-2.,-2.,-2.],[-1.,-1.,-1.]],
        [[-2.,-1.,0.],[-2.,-1.,0.],[-1.,0.,0.]]]],dtype=float32), 
array([[[[-1.,0.,0.]],[[-1.,-1.,0.]]],
  [[[ 0.,-1.,-1.]],[[0.,0.,-1.]]]],dtype=float32)]</pre>
<hr class="calibre6"/>
<p class="ziti3">上面输出中，生成的多通道的输出，是按列排列的（每一个feature map为一列）。为了方便理解，以op2为例，其剖析图如图8-8所示。</p>
<div class="pic"><img alt="" src="Image00134.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-8　卷积示例</p>
<p class="ziti3">（3）执行代码8-1中的33～39行，对应的输出如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">op4:
 [array([[[[-4.,-4.],[-4.,-4.],[-2.,-2.]],
        [[-4.,-4.],[-4.,-4.],[-2.,-2.]],
        [[-2.,-2.],[-2.,-2.],[-1.,-1.]]]],dtype=float32), 
array([[[[-1.,0.],[0.,-1.]],[[-1.,0.],[0.,-1.]]],
 [[[-1.,0.],[0.,-1.]],[[-1.,0.],[0.,-1.]]]],dtype=float32)]
op5:
 [array([[[[-4.],[-4.],[-2.]],
        [[-4.],[-4.],[-2.]],
        [[-2.],[-2.],[-1.]]]],dtype=float32), 
array([[[[-1.],[0.]],[[0.],[-1.]]],
 [[[-1.],[0.]],[[0.],[-1.]]]],dtype=float32)]</pre>
<hr class="calibre6"/>
<p class="ziti3">卷积核对多通道输入的卷积处理，是多通道的结果叠加。以op5为例展开。图8-9所示为将每个通道的feature map叠加生成了最终的结果。</p>
<div class="pic"><img alt="" src="Image00135.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-9　多通道卷积</p>
<p class="ziti3">（4）执行代码8-1中的42～45行，对应的输出如下，是不同尺寸输入分别为SAME和VALID时的比较。</p>
<hr class="calibre6"/>
<pre class="ziti5">op1:
 [array([[[[-2.],[-2.],[-1.]],
        [[-2.],[-2.],[-1.]],
 [[-1.],[-1.],[-1.]]]],dtype=float32), 
array([[[[-1.]],[[0.]]],
       [[[0.]],[[-1.]]]],dtype=float32)]
vop1:
 [array([[[[-2.],[-2.]],
        [[-2.],[-2.]]]],dtype=float32), 
array([[[[-1.]],[[0.]]],
       [[[0.]],[[-1.]]]],dtype=float32)] 
op6:
 [array([[[[-2.],[-2.]],
        [[-2.],[-2.]]]],dtype=float32), 
array([[[[-1.]],[[0.]]],
       [[[0.]],[[-1.]]]],dtype=float32)]
vop6:
 [array([[[[-2.],[-2.]],
        [[-2.],[-2.]]]],dtype=float32), 
array([[[[-1.]],[[0.]]],
       [[[0.]],[[-1.]]]],dtype=float32)]</pre>
<hr class="calibre6"/>
<p class="ziti3">通过上面的结果可以看出：</p>
<p class="ziti4">·对于op1和vop1的比较可以看出5×5矩阵在padding为'SAME'时生成的是3×3矩阵，而在'VALID'时生成的是2×2。</p>
<p class="ziti4">·而在op6和vop6的例子中，对于4×4矩阵在padding为'SAME'和'VALID'下都会生成2×2的矩阵，这是因为4×4的输入对于2×2的卷积核步长为2的情况下，正好可以把所有数据处理完，本身在'SAME'的情况下就不需要补0。</p>
<p class="ziti3">通过卷积函数可以实现8.4.2节卷积操作中的窄卷积和同卷积（步长唯一并且补零操作的卷积），但不能实现全卷积。TensorFlow中有单独的反卷积函数，会在后面会讲到。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 本节特意用了很多篇幅来解释卷积的操作细节，表明这部分内容非常重要，是卷积神经网络的重点。将卷积操作的细节理解透彻，会使你在实际编程过程中少走弯路。因为在自己搭建网络的过程中，必须对输入、输出的具体维度有个清晰的计算，这样才能保证网络结构的正确性，才能使网络运行下去。</p>
<h4 class="p3">8.4.4　实例38：使用卷积提取图片的轮廓</h4>
<p class="ziti3">通过8.4.3节的练习，相信读者已经掌握了卷积操作的细节。下面来做一个实际的例子，通过卷积操作来实现本章开篇所讲的sobel算子。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">通过卷积操作来实现本章开篇所讲的sobel算子，将彩色的图片生成带有边缘化信息的图片。</p>
<p class="ziti3">本例中先载入一个图片，然后使用一个“3通道输入，1通道输出的3×3卷积核”（即sobel算子），最后使用卷积函数输出生成的结果。</p>
<p class="ziti4">
<span class="yanse">1．载入图片并显示</span>
</p>
<p class="ziti3">首先将图片放到代码的同级目录下，通过imread载入，然后将其显示并打印出来。</p>
<p class="ziti3">代码8-2　sobel</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import matplotlib.pyplot as plt # plt 用于显示图片
02 import matplotlib.image as mpimg # mpimg 用于读取图片
03 import numpy as np
04 import tensorflow as tf  
05 
06 myimg = mpimg.imread('img.jpg') # 读取和代码处于同一目录下的图片
07 plt.imshow(myimg)                 # 显示图片
08 plt.axis('off')                     # 不显示坐标轴
09 plt.show()
10 print(myimg.shape)</pre>
<hr class="calibre6"/>
<p class="ziti3">运行上面代码，得到输出如下，输出图片如图8-10所示。</p>
<div class="pic"><img alt="" src="Image00136.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-10　图片显示</p>
<hr class="calibre6"/>
<pre class="ziti5">(3264, 2448, 3)</pre>
<hr class="calibre6"/>
<p class="ziti3">可以看到，载入的图片维度为3264×2448大小，3个通道。</p>
<p class="ziti4">
<span class="yanse">2．定义占位符、卷积核、卷积op</span>
</p>
<p class="ziti3">这里需要手动将sobel算子填入到卷积核里。使用tf.constant函数可以将常量直接初始化到Variable中，因为是3通道，所以sobel卷积核的每个元素都扩成了3个。</p>
<p class="ziti4">
<span class="yanse"><img alt="" class="formula-2em" src="Image00014.jpg"/>
 注意：</span>
 sobel算子处理过的图片不保证每个像素都在0～255之间，所以要做一次归一化操作（即用每个值减去最小值的结果，再除以最大值与最小值的差），让生成的值都在[0，1]之间，然后再乘以255。</p>
<p class="ziti3">代码8-2　sobel（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">11 full=np.reshape(myimg,[1,3264,2448,3])  
12 inputfull = tf.Variable(tf.constant(1.0,shape = [1, 3264, 2448, 3]))
13 
14 filter = tf.Variable(tf.constant([[-1.0,-1.0,-1.0], [0,0,0], [1.0,1.0,1.0],
15                              [-2.0,-2.0,-2.0], [0,0,0],  [2.0,2.0,2.0],
16                              [-1.0,-1.0,-1.0],[0,0,0],[1.0,1.0,1.0]],
                                  shape = [3, 3, 3, 1])) 
17 
18 op= tf.nn.conv2d(inputfull, filter, strides=[1, 1, 1, 1], padding='SAME')
  #3个通道输入，生成1个feature ma
19 o=tf.cast(  ((op-tf.reduce_min(op))/(tf.reduce_max(op)-tf.reduce_min
(op)) ) *255 ,tf.uint8)</pre>
<hr class="calibre6"/>
<p class="ziti3">上面的代码中，卷积op的步长为1×1，padding为SAME表明这是个同卷积的操作。</p>
<p class="ziti4">
<span class="yanse">3．运行卷积操作并显示</span>
</p>
<p class="ziti3">现在就可以建立session然后运行程序了。具体代码如下。</p>
<p class="ziti3">代码8-2　sobel（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">20 with tf.Session() as sess:  
21     sess.run(tf.global_variables_initializer()  )  
22
￼23     t,f=sess.run([o,filter],feed_dict={ inputfull:
    full})
24    
25     t=np.reshape(t,[3264,2448]) 
26 
27     plt.imshow(t,cmap='Greys_r') #显示图片
28     plt.axis('off')                  #不显示坐标轴
29     plt.show()</pre>
<hr class="calibre6"/>
<p class="ziti3">上述代码执行后输出结果如图8-11所示。</p>
<div class="pic"><img alt="" src="Image00137.jpg" class="calibre4"/>
</div>
<p class="middle-img">图8-11　边缘化</p>
<p class="ziti3">可以看出，sobel的卷积操作之后，提取到了一张含有轮廓特征的图像。</p>
<h4 class="p3">8.4.5　池化函数tf.nn.max_pool（avg_pool）</h4>
<p class="ziti3">TensorFlow里的池化函数如下：</p>
<hr class="calibre6"/>
<pre class="ziti5">tf.nn.max_pool(input, ksize, strides, padding, name=None)
tf.nn.avg_pool(input, ksize, strides, padding, name=None)</pre>
<hr class="calibre6"/>
<p class="ziti3">这两个函数中的4个参数和卷积参数很类似，具体说明如下。</p>
<p class="ziti4">·value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch，height，width，channels]这样的shape。</p>
<p class="ziti4">·ksize：池化窗口的大小，取一个四维向量，一般是[1，height，width，1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1。</p>
<p class="ziti4">·strides：和卷积参数含义类似，窗口在每一个维度上滑动的步长，一般也是[1，stride，stride，1]。</p>
<p class="ziti4">·padding：和卷积参数含义一样，也是取VALID或者SAME，VALID是不padding操作，SAME是padding操作。</p>
<p class="ziti4">·返回一个Tensor，类型不变，shape仍然是[batch，height，width，channels]这种形式。</p>
<h4 class="p3">8.4.6　实例39：池化函数的使用</h4>
<p class="ziti3">下面通过一个例子来介绍池化函数的用法。</p>
<p class="ziti3">
<span class="yanse">实例描述</span>
</p>
<p class="ziti3">通过手动生成一个4×4的矩阵来模拟图片，定义一个2×2的滤波器，通过几个在卷积神经网络中常用的池化操作来测试池化函数里的参数，并验证输出结果。</p>
<p class="ziti4">
<span class="yanse">1．定义输入变量</span>
</p>
<p class="ziti3">定义1个输入变量用来模拟输入图片、4×4大小的2通道矩阵，并将其赋予指定的值。2个通道分别为：4个0到3 3 3 3 组成的矩阵，4个4到7 7 7 7组成的矩阵。</p>
<p class="ziti3">代码8-3　池化函数使用</p>
<hr class="calibre6"/>
<pre class="ziti5">01 import tensorflow as tf  
02   
03 img=tf.constant([  
04         [[0.0,4.0],[0.0,4.0],[0.0,4.0],[0.0,4.0]],  
05         [[1.0,5.0],[1.0,5.0],[1.0,5.0],[1.0,5.0]],  
06         [[2.0,6.0],[2.0,6.0],[2.0,6.0],[2.0,6.0]],  
07         [[3.0,7.0],[3.0,7.0], [3.0,7.0],[3.0,7.0]]
08     ])
09 img=tf.reshape(img,[1,4,4,2])</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">2．定义池化操作</span>
</p>
<p class="ziti3">这里定义了4个池化操作和一个取均值操作。前两个操作是最大池化操作，接下来是两个均值池化操作，最后一个是取均值操作。</p>
<p class="ziti3">代码8-3　池化函数使用（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">10 pooling=tf.nn.max_pool(img,[1,2,2,1],[1,2,2,1],padding='VALID')  
11 pooling1=tf.nn.max_pool(img,[1,2,2,1],[1,1,1,1],padding='VALID')
12 pooling2=tf.nn.avg_pool(img,[1,4,4,1],[1,1,1,1],padding='SAME')  
13 pooling3=tf.nn.avg_pool(img,[1,4,4,1],[1,4,4,1],padding='SAME') 
14 nt_hpool2_flat = tf.reshape(tf.transpose(img), [-1, 16]) 
15 pooling4=tf.reduce_mean(nt_hpool2_flat,1) #1表示对行求均值（1表示轴是列），
0表示对列求均值</pre>
<hr class="calibre6"/>
<p class="ziti4">
<span class="yanse">3．运行池化操作</span>
</p>
<p class="ziti3">在本步骤操作之前，读者可以把前面的规则熟记一下，然后试着自己推导一下，比较得到的输出结果。下面把这些结果打印出来，看看与你推导的是否一致。</p>
<p class="ziti3">代码8-3　池化函数使用（续）</p>
<hr class="calibre6"/>
<pre class="ziti5">16 with tf.Session() as sess:  
17     print("image:")  
18     image=sess.run(img)  
19     print (image)  
20     result=sess.run(pooling)  
21     print ("reslut:\n",result)  
22     result=sess.run(pooling1)  
23     print ("reslut1:\n",result)     
24     result=sess.run(pooling2)  
25     print ("reslut2:\n",result)
26     result=sess.run(pooling3)  
27     print ("reslut3:\n",result) 
28     flat,result=sess.run([nt_hpool2_flat,pooling4])  
29     print ("reslut4:\n",result) 
30     print("flat:\n",flat)</pre>
<hr class="calibre6"/>
<p class="ziti3">执行上面的代码，得到如下输出（为了方便读者观看，这里将格式进行了整理）：</p>
<hr class="calibre6"/>
<pre class="ziti5">image:
[[[[ 0.  4.]
   [ 0.  4.]
   [ 0.  4.]
   [ 0.  4.]]
 
  [[ 1.  5.]
   [ 1.  5.]
   [ 1.  5.]
   [ 1.  5.]]
 
  [[ 2.  6.]
   [ 2.  6.]
   [ 2.  6.]
   [ 2.  6.]]
 
  [[ 3.  7.]
   [ 3.  7.]
   [ 3.  7.]
   [ 3.  7.]]]]</pre>
<hr class="calibre6"/>
<p class="ziti3">通过上面的输出可以看出，img与我们设置的初始值是一样的，即第一个通道为[[0 0 0 0]，[1 1 1 1]，[2 2 2 2]，[3 3 3 3]]；第二个通道为[[4 4 4 4]，[5 5 5 5]，[6 6 6 6]，[7 7 7 7]]。</p>
<hr class="calibre6"/>
<pre class="ziti5">reslut:
 [[[[ 1.  5.]
   [ 1.  5.]]
 
  [[ 3.  7.]
   [ 3.  7.]]]]</pre>
<hr class="calibre6"/>
<p class="ziti3">这个操作在卷积神经网络中是最常用的，一般步长都会设成与池化滤波器尺寸一致（池化的卷积尺寸为2×2，所以步长也是2），生成2个通道的2×2矩阵。矩阵的内容是从原始输入中取最大值，由于池化filter中对应通道的维度是1，所以结果仍然保持源通道数。</p>
<hr class="calibre6"/>
<pre class="ziti5">reslut1:
 [[[[ 1.  5.]
   [ 1.  5.]
   [ 1.  5.]]
 
  [[ 2.  6.]
   [ 2.  6.]
   [ 2.  6.]]
 
  [[ 3.  7.]
   [ 3.  7.]
   [ 3.  7.]]]]
reslut2:
 [[[[ 1.   5. ]
   [ 1.   5. ]
   [ 1.   5. ]
   [ 1.   5. ]]
 
  [[ 1.5  5.5]
   [ 1.5  5.5]
   [ 1.5  5.5]
   [ 1.5  5.5]]
 
  [[ 2.   6. ]
   [ 2.   6. ]
   [ 2.   6. ]
   [ 2.   6. ]]
 
  [[ 2.5  6.5]
   [ 2.5  6.5]
   [ 2.5  6.5]
   [ 2.5  6.5]]]]</pre>
<hr class="calibre6"/>
<p class="ziti3">result1和result2分别演示了VALID和SAME的两种pading的取值。</p>
<p class="ziti4">·VALID中使用的filter为2×2，步长为1×1，生成了2×2大小的矩阵。</p>
<p class="ziti4">·在SAME中使用的4×4的filter，步长仍然为1×1，生成了4×4的矩阵，padding之后在计算avg_pool时，是将输入矩阵与filter对应尺寸内的元素总和除以这些元素中非零的个数（而不是filter的总个数）。</p>
<hr class="calibre6"/>
<pre class="ziti5">reslut3:
 [[[[ 1.5  5.5]]]]
reslut4:
 [ 1.5  5.5]
flat:
 [[ 0.  1.  2.  3.  0.  1.  2.  3.  0.  1.  2.  3.  0.  1.  2.  3.]
 [ 4.  5.  6.  7.  4.  5.  6.  7.  4.  5.  6.  7.  4.  5.  6.  7.]]</pre>
<hr class="calibre6"/>
<p class="ziti3">result3是常用的操作手法，也叫全局池化法，就是使用一个与原有输入同样尺寸的filter进行池化，一般放在最后一层，用于表达图像通过卷积网络处理后的最终特征。而result4是一个均值操作，可以看到将数据转置后的均值操作得到的值与全局池化平均值是一样的结果。</p>
<div class="calibre1">本书由「<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a>」整理，<a href="https://epubw.com" class="pcalibre calibre2">ePUBw.COM</a> 提供最新最全的优质电子书下载！！！</div></body>
</html>
